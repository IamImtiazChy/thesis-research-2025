{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IamImtiazChy/thesis-research-2025/blob/main/Assertion_Accuracy_Framework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kPTismzTR4S"
      },
      "source": [
        "# A Framework for Maximizing Assertion Accuracy in LLM-Generated Test Code\n",
        "## 1. Introduction and Setup\n",
        "\n",
        "This notebook implements a framework for maximizing assertion accuracy in LLM-generated test code. It covers data collection from GitHub, HumanEval+, and MBPP, test generation using DeepSeek and Microsoft Phi LLMs, mutation testing for assertion evaluation, and an assertion correction mechanism.\n",
        "\n",
        "**Research Objectives:**\n",
        "1.  Collect and preprocess code and test data from diverse sources.\n",
        "2.  Utilize DeepSeek and Phi LLMs (downloadable models) to generate test cases for Python functions.\n",
        "3.  Evaluate the quality of LLM-generated assertions using mutation testing.\n",
        "4.  Develop and apply a model/technique to correct or refine weak/incorrect assertions.\n",
        "5.  Analyze the overall effectiveness of the framework.\n"
      ],
      "id": "0kPTismzTR4S"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUdppO3dTR4W"
      },
      "source": [
        "### 1.1 Install Necessary Libraries\n"
      ],
      "id": "RUdppO3dTR4W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "idvZQUpqTR4W"
      },
      "outputs": [],
      "source": [
        "!pip install -q PyGithub pandas datasets transformers accelerate bitsandbytes pyarrow torch\n"
      ],
      "id": "idvZQUpqTR4W"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdiFNRm4TR4X"
      },
      "source": [
        "### 1.2 Import Required Modules\n"
      ],
      "id": "TdiFNRm4TR4X"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJScRop9TR4Y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from github import Github\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from datasets import load_dataset\n",
        "import ast\n",
        "import importlib.util\n",
        "import sys\n",
        "import traceback\n",
        "import random\n",
        "import time\n",
        "from IPython.display import display, Markdown\n"
      ],
      "id": "XJScRop9TR4Y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ak3wgkfTR4Y"
      },
      "source": [
        "### 1.3 Mount Google Drive\n"
      ],
      "id": "-ak3wgkfTR4Y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMelbPUUTR4Y"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Define base path for project data on Google Drive\n",
        "BASE_DRIVE_PATH = \"/content/drive/MyDrive/Colab Notebooks/Assertion-Accuracy-Research/Data\"\n",
        "os.makedirs(BASE_DRIVE_PATH, exist_ok=True)\n",
        "print(f\"Data will be stored in: {BASE_DRIVE_PATH}\")\n"
      ],
      "id": "bMelbPUUTR4Y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28u6WECiTR4Z"
      },
      "source": [
        "### 1.4 GitHub Authentication (using Colab Secrets)\n"
      ],
      "id": "28u6WECiTR4Z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2E5SvzSXTR4Z"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "g = None\n",
        "try:\n",
        "    ACCESS_TOKEN = userdata.get('GITHUB_TOKEN')\n",
        "    if not ACCESS_TOKEN:\n",
        "        print(\"GitHub Access Token not found in Colab Secrets. Please add it as 'GITHUB_TOKEN'.\")\n",
        "        print(\"GitHub data collection will be skipped if token is not available.\")\n",
        "    else:\n",
        "        g = Github(ACCESS_TOKEN)\n",
        "        github_user = g.get_user()\n",
        "        print(f\"Successfully authenticated to GitHub as: {github_user.login}\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not retrieve GITHUB_TOKEN from Colab Secrets or authentication failed: {e}\")\n",
        "    print(\"GitHub data collection will be skipped.\")\n"
      ],
      "id": "2E5SvzSXTR4Z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTYxvU_7TR4Z"
      },
      "source": [
        "## 2. Data Collection and Preprocessing\n",
        "\n",
        "This section covers collecting data from GitHub repositories, HumanEval+, and MBPP datasets.\n"
      ],
      "id": "qTYxvU_7TR4Z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eqnIBjtTR4a"
      },
      "source": [
        "### 2.1 GitHub Data Collection\n",
        "This cell contains the functions to extract data from GitHub. It will only run if the GitHub token was successfully loaded.\n"
      ],
      "id": "-eqnIBjtTR4a"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PyGithub and Import Libraries\n",
        "\"\"\"\n",
        "!pip install PyGithub pandas\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from github import Github\n",
        "import os"
      ],
      "metadata": {
        "id": "Gf7TQQ-QXIDP"
      },
      "id": "Gf7TQQ-QXIDP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GitHub Authentication and Helper Functions\n",
        "\n",
        "from google.colab import userdata\n",
        "ACCESS_TOKEN = userdata.get(\"GITHUB_TOKEN\")\n",
        "\n",
        "if not ACCESS_TOKEN:\n",
        "    print(\"GitHub Access Token not found. Please set it up in Colab Secrets as GITHUB_TOKEN.\")\n",
        "    # Or raise an error / stop execution\n",
        "    g = None\n",
        "else:\n",
        "    try:\n",
        "        g = Github(ACCESS_TOKEN)\n",
        "        user = g.get_user()\n",
        "        print(f\"Successfully authenticated to GitHub as: {user.login}\")\n",
        "    except Exception as e:\n",
        "        print(f\"GitHub authentication failed: {e}\")\n",
        "        g = None\n",
        "\n",
        "def extract_functions_tests_errors(code):\n",
        "    \"\"\"Extract functions, test cases, and error messages (assertions) from code.\"\"\"\n",
        "    # Adjusted patterns for better matching and to avoid overly greedy matches\n",
        "    # Function pattern: def function_name(params): ... (multiline, non-greedy body)\n",
        "    function_pattern = r\"def\\s+(?!test_)\\w+\\(.*?\\):(?:\\s*\\\"\\\"\\\"[^\\\"]*\\\"\\\"\\\"|\\s*\\'\\'\\'.*?\\'\\'\\')?\\s*\\n(?:^\\s{4,}.*\\n|\\s*\\n)*\"\n",
        "    # Test pattern: def test_function_name(params): ... (multiline, non-greedy body)\n",
        "    test_pattern = r\"def\\s+test_\\w+\\(.*?\\):(?:\\s*\\\"\\\"\\\"[^\\\"]*\\\"\\\"\\\"|\\s*\\'\\'\\'.*?\\'\\'\\')?\\s*\\n(?:^\\s{4,}.*\\n|\\s*\\n)*\"\n",
        "    # Assertion pattern: assert ... (single line or simple multiline)\n",
        "    # This pattern is simplified; robust assertion extraction can be complex.\n",
        "    assertion_pattern = r\"^\\s*assert .*?(?=\\n\\s*[^\\s#]|$)\" # Matches assert until end of line or next non-indented line\n",
        "\n",
        "    functions = re.findall(function_pattern, code, re.MULTILINE)\n",
        "    test_cases = re.findall(test_pattern, code, re.MULTILINE)\n",
        "    assertions = re.findall(assertion_pattern, code, re.MULTILINE)\n",
        "\n",
        "    return {\n",
        "        \"functions\": functions,\n",
        "        \"test_cases\": test_cases,\n",
        "        \"assertions\": assertions, # Renamed from error_messages for clarity\n",
        "    }\n",
        "\n",
        "def process_repository(repo_name):\n",
        "    \"\"\"Process a single repository and extract code components.\"\"\"\n",
        "    if not g:\n",
        "        print(f\"GitHub client not initialized. Skipping repository {repo_name}.\")\n",
        "        return []\n",
        "    try:\n",
        "        print(f\"Fetching repository: {repo_name}\")\n",
        "        repo = g.get_repo(repo_name)\n",
        "        repo_data = []\n",
        "        contents_to_process = []\n",
        "\n",
        "        # Initial contents from the root directory\n",
        "        try:\n",
        "            root_contents = repo.get_contents(\"\")\n",
        "            contents_to_process.extend(root_contents)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not get root contents for {repo_name}: {e}\")\n",
        "            return []\n",
        "\n",
        "        processed_paths = set()\n",
        "        file_count = 0\n",
        "        max_files_to_process = 200 # Limit to avoid very long processing times in Colab for large repos\n",
        "\n",
        "        while contents_to_process and file_count < max_files_to_process:\n",
        "            file_content = contents_to_process.pop(0)\n",
        "\n",
        "            if file_content.path in processed_paths:\n",
        "                continue\n",
        "            processed_paths.add(file_content.path)\n",
        "\n",
        "            if file_content.type == \"dir\":\n",
        "                try:\n",
        "                    print(f\"  Exploring directory: {file_content.path}\")\n",
        "                    dir_contents = repo.get_contents(file_content.path)\n",
        "                    contents_to_process.extend(dir_contents)\n",
        "                except Exception as e:\n",
        "                    print(f\"  Warning: Could not get contents of dir {file_content.path}: {e}\")\n",
        "            elif file_content.name.endswith(\".py\"):\n",
        "                file_count += 1\n",
        "                print(f\"  Processing file ({file_count}): {file_content.path}\")\n",
        "                try:\n",
        "                    # Check if file is too large to avoid memory issues\n",
        "                    if file_content.size > 1000000: # 1MB limit for individual files\n",
        "                        print(f\"    Skipping large file: {file_content.path} (size: {file_content.size} bytes)\")\n",
        "                        continue\n",
        "                    code = file_content.decoded_content.decode(\"utf-8\", errors=\"ignore\")\n",
        "                    extracted = extract_functions_tests_errors(code)\n",
        "\n",
        "                    if any(extracted.values()): # If any list (functions, tests, assertions) is not empty\n",
        "                        data = {\n",
        "                            \"repo_name\": repo_name,\n",
        "                            \"file_path\": file_content.path,\n",
        "                            \"functions_count\": len(extracted[\"functions\"]),\n",
        "                            \"test_cases_count\": len(extracted[\"test_cases\"]),\n",
        "                            \"assertions_count\": len(extracted[\"assertions\"]),\n",
        "                            \"raw_functions\": extracted[\"functions\"],\n",
        "                            \"raw_test_cases\": extracted[\"test_cases\"],\n",
        "                            \"raw_assertions\": extracted[\"assertions\"],\n",
        "                        }\n",
        "                        repo_data.append(data)\n",
        "                except Exception as e:\n",
        "                    print(f\"    Error processing file {file_content.path}: {str(e)}\")\n",
        "\n",
        "        if file_count >= max_files_to_process:\n",
        "            print(f\"Reached max file processing limit ({max_files_to_process}) for repository {repo_name}.\")\n",
        "        return repo_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing repository {repo_name}: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def create_github_dataset(repositories, drive_output_path):\n",
        "    \"\"\"Create structured dataset from list of repositories and save to Google Drive.\"\"\"\n",
        "    all_data = []\n",
        "    total_repos = len(repositories)\n",
        "\n",
        "    for idx, repo_name in enumerate(repositories, 1):\n",
        "        print(f\"\\nProcessing repository [{idx}/{total_repos}]: {repo_name}\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        repo_data = process_repository(repo_name)\n",
        "        all_data.extend(repo_data)\n",
        "\n",
        "        print(f\"Completed {repo_name}: Found {len(repo_data)} Python files with relevant code snippets.\")\n",
        "        print(f\"Progress: {idx/total_repos*100:.1f}%\")\n",
        "\n",
        "    if not all_data:\n",
        "        print(\"\\nNo data collected from GitHub repositories.\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\nCreating final GitHub dataset...\")\n",
        "    df = pd.DataFrame(all_data)\n",
        "\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(drive_output_path), exist_ok=True)\n",
        "        df.to_csv(drive_output_path, index=False)\n",
        "        print(f\"\\nGitHub dataset saved successfully to {drive_output_path}\")\n",
        "        print(f\"Total Python files processed and included: {len(df)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError saving GitHub dataset to {drive_output_path}: {e}\")\n",
        "        print(\"Displaying DataFrame head as fallback:\")\n",
        "        print(df.head())\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "GmbjRIYLXXdG"
      },
      "id": "GmbjRIYLXXdG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "B4h6N2mZTR4a"
      },
      "outputs": [],
      "source": [
        "# Run GitHub Data Collection\n",
        "print(\"Running GitHub Data Collection for Colab...\")\n",
        "# For the full research, expand this list or use a more systematic way to select repos.\n",
        "repositories_to_process = [\"pallets/flask\", \"requests/requests\"]\n",
        "\n",
        "# Define output path in Google Drive\n",
        "github_csv_path = \"/content/drive/MyDrive/Colab Notebooks/Assertion-Accuracy-Research/Data/github_dataset.csv\"\n",
        "\n",
        "github_df = create_github_dataset(repositories_to_process, github_csv_path)\n",
        "\n",
        "if github_df is not None:\n",
        "    print(\"\\nGitHub Dataset Statistics:\")\n",
        "    print(f\"Total number of files processed: {len(github_df)}\")\n",
        "    print(f\"Total functions extracted: {github_df['functions_count'].sum()}\")\n",
        "    print(f\"Total test cases extracted: {github_df['test_cases_count'].sum()}\")\n",
        "    print(f\"Total assertions found: {github_df['assertions_count'].sum()}\")\n",
        "    print(\"\\nGitHub Dataset Preview (first 5 rows):\")\n",
        "    from IPython.display import display # For Colab\n",
        "    display(github_df.head())\n",
        "elif g is None:\n",
        "    print(\"Skipping GitHub data collection as GitHub client is not initialized.\")\n",
        "else:\n",
        "    print(\"This script is intended to be run in a Google Colab environment or adapted for local use.\")"
      ],
      "id": "B4h6N2mZTR4a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SukrHEltTR4b"
      },
      "source": [
        "### 2.2 HumanEval+ Dataset Collection\n"
      ],
      "id": "SukrHEltTR4b"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install/Ensure pandas and pyarrow are available\n",
        "\"\"\"\n",
        "!pip install pandas pyarrow\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import re"
      ],
      "metadata": {
        "id": "Siawf2FDYrag"
      },
      "id": "Siawf2FDYrag",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to extract assertions\n",
        "def extract_assertions_from_test_code(test_code_str):\n",
        "    \"\"\"Extracts assertion statements from a string of test code using regex.\"\"\"\n",
        "    # This is a simplified regex. AST parsing would be more robust but adds complexity.\n",
        "    assertion_pattern = r\"^\\s*assert .*?(?=\\n\\s*[^\\s#]|$)\" # Matches assert until end of line or next non-indented line\n",
        "    assertions = re.findall(assertion_pattern, test_code_str, re.MULTILINE)\n",
        "    return assertions\n"
      ],
      "metadata": {
        "id": "CWej2XS0ZImh"
      },
      "id": "CWej2XS0ZImh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and Process HumanEval+ Dataset\n",
        "def load_and_process_humanevalplus(drive_output_path):\n",
        "    \"\"\"Loads HumanEval+ dataset, processes it, and saves to Google Drive.\"\"\"\n",
        "    print(\"Loading HumanEval+ dataset from Hugging Face Hub...\")\n",
        "    try:\n",
        "        parquet_url = \"hf://datasets/evalplus/humanevalplus/data/test-00000-of-00001-5973903632b82d40.parquet\"\n",
        "        df = pd.read_parquet(parquet_url)\n",
        "        print(f\"Successfully loaded HumanEval+ dataset. Shape: {df.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading HumanEval+ dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "    processed_data = []\n",
        "    print(\"Processing HumanEval+ data...\")\n",
        "    for index, row in df.iterrows():\n",
        "        try:\n",
        "            task_id = row.get(\"task_id\", f\"humaneval_{index}\")\n",
        "            # Combine prompt and canonical_solution to form the full function code\n",
        "            function_code = row.get(\"prompt\", \"\") + row.get(\"canonical_solution\", \"\")\n",
        "            test_code = row.get(\"test\", \"\")\n",
        "            entry_point = row.get(\"entry_point\", \"\")\n",
        "\n",
        "            if not function_code.strip() or not test_code.strip():\n",
        "                print(f\"Skipping entry {task_id} due to missing function or test code.\")\n",
        "                continue\n",
        "\n",
        "            # Ensure function_code is a complete, parseable function definition\n",
        "            # The prompt often is `def func_name():\\n` and solution is `  return ...`\n",
        "            # We might need to ensure proper indentation if solution doesn't start with it.\n",
        "            # For simplicity, we assume they combine correctly here.\n",
        "\n",
        "            assertions = extract_assertions_from_test_code(test_code)\n",
        "\n",
        "            processed_data.append({\n",
        "                \"dataset_source\": \"HumanEval+\",\n",
        "                \"function_id\": task_id,\n",
        "                \"function_code\": function_code.strip(),\n",
        "                \"test_code\": test_code.strip(),\n",
        "                \"assertions\": assertions,\n",
        "                \"assertions_count\": len(assertions),\n",
        "                \"entry_point\": entry_point\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing row {index} from HumanEval+: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not processed_data:\n",
        "        print(\"No data processed from HumanEval+.\")\n",
        "        return None\n",
        "\n",
        "    processed_df = pd.DataFrame(processed_data)\n",
        "    print(f\"Successfully processed {len(processed_df)} entries from HumanEval+.\")\n",
        "\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(drive_output_path), exist_ok=True)\n",
        "        processed_df.to_csv(drive_output_path, index=False)\n",
        "        print(f\"Processed HumanEval+ dataset saved to {drive_output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving processed HumanEval+ dataset to {drive_output_path}: {e}\")\n",
        "        print(\"Displaying DataFrame head as fallback:\")\n",
        "        print(processed_df.head())\n",
        "\n",
        "    return processed_df\n"
      ],
      "metadata": {
        "id": "mvmK3cZAZX1w"
      },
      "id": "mvmK3cZAZX1w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84d204sQTR4b"
      },
      "outputs": [],
      "source": [
        "# Run HumanEval+ Data Collection and Processing\n",
        "print(\"Running HumanEval+ Data Collection and Processing for Colab...\")\n",
        "\n",
        "humaneval_csv_path = \"/content/drive/MyDrive/Colab Notebooks/Assertion-Accuracy-Research/Data/humanevalplus_dataset.csv\"\n",
        "\n",
        "humaneval_df = load_and_process_humanevalplus(humaneval_csv_path)\n",
        "\n",
        "if humaneval_df is not None:\n",
        "    print(\"\\nHumanEval+ Processed Dataset Statistics:\")\n",
        "    print(f\"Total number of entries: {len(humaneval_df)}\")\n",
        "    print(f\"Total assertions extracted: {humaneval_df['assertions_count'].sum()}\")\n",
        "    print(\"\\nHumanEval+ Processed Dataset Preview (first 5 rows):\")\n",
        "    from IPython.display import display # For Colab\n",
        "    display(humaneval_df.head())\n",
        "else:\n",
        "    print(\"This script is intended to be run in a Google Colab environment or adapted for local use.\")"
      ],
      "id": "84d204sQTR4b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq-Vb8hHTR4b"
      },
      "source": [
        "### 2.3 MBPP Dataset Collection\n"
      ],
      "id": "Yq-Vb8hHTR4b"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install/Ensure datasets, pandas are available\n",
        "\"\"\"\n",
        "!pip install -q datasets pandas\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import ast\n",
        "import os\n",
        "import json\n",
        "import re"
      ],
      "metadata": {
        "id": "fZZwzw7oa3mN"
      },
      "id": "fZZwzw7oa3mN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and Process MBPP Dataset\n",
        "\n",
        "# Helper function to extract assertions from test code string\n",
        "def extract_assertions_from_test(test_code_str):\n",
        "    assertions = []\n",
        "    if not isinstance(test_code_str, str):\n",
        "        return assertions # Return empty if not a string\n",
        "    try:\n",
        "        tree = ast.parse(test_code_str)\n",
        "        for node in ast.walk(tree):\n",
        "            if isinstance(node, ast.Assert):\n",
        "                try:\n",
        "                    # ast.unparse requires Python 3.9+. Colab usually has a recent version.\n",
        "                    assertion_text = ast.unparse(node.test).strip()\n",
        "                except AttributeError:\n",
        "                    # Basic fallback for older ast or if unparse is not available/fails\n",
        "                    assertion_text = f\"Assertion involving {type(node.test.ops[0]).__name__ if hasattr(node.test, 'ops') and node.test.ops else 'comparison'}\"\n",
        "                assertions.append(assertion_text)\n",
        "    except SyntaxError:\n",
        "        # print(f\"SyntaxError while parsing test code for assertions: {test_code_str[:100]}...\")\n",
        "        pass # Or log an error\n",
        "    return assertions\n",
        "\n",
        "def load_and_process_mbpp(output_csv_path):\n",
        "    \"\"\"\n",
        "    Loads the MBPP dataset directly from Hugging Face Parquet files using pandas,\n",
        "    processes it to extract relevant information, and saves it to a CSV file.\n",
        "    This method is more robust against Colab caching issues with the datasets library.\n",
        "    \"\"\"\n",
        "    print(\"Loading MBPP dataset directly from Hugging Face Parquet files (google-research-datasets/mbpp)...\")\n",
        "\n",
        "    # Using the 'test' split as per previous requirements and user example\n",
        "    # The user provided: df = pd.read_parquet(\"hf://datasets/google-research-datasets/mbpp/\" + splits[\"train\"])\n",
        "    # We need the 'test' split: 'full/test-00000-of-00001.parquet'\n",
        "    # The dataset name on HF Hub is \"google/mbpp\" or \"google-research-datasets/mbpp\"\n",
        "    # The user's example used \"google-research-datasets/mbpp\"\n",
        "    # The structure from user: 'task_id', 'text', 'code', 'test_list', 'test_setup_code', 'challenge_test_list'\n",
        "\n",
        "    mbpp_parquet_url = \"hf://datasets/google-research-datasets/mbpp/full/test-00000-of-00001.parquet\"\n",
        "    # Alternative if the above doesn't work, sometimes it's just 'mbpp' as dataset name\n",
        "    # mbpp_parquet_url_alt = \"hf://datasets/mbpp/full/test-00000-of-00001.parquet\"\n",
        "\n",
        "    try:\n",
        "        print(f\"Attempting to load MBPP test split from: {mbpp_parquet_url}\")\n",
        "        mbpp_df = pd.read_parquet(mbpp_parquet_url)\n",
        "        print(\"MBPP dataset (test split) loaded successfully via pandas.read_parquet.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading MBPP dataset from {mbpp_parquet_url}: {e}\")\n",
        "        # print(f\"Attempting to load MBPP test split from alternative URL: {mbpp_parquet_url_alt}\")\n",
        "        # try:\n",
        "        #     mbpp_df = pd.read_parquet(mbpp_parquet_url_alt)\n",
        "        #     print(\"MBPP dataset (test split) loaded successfully via pandas.read_parquet from alternative URL.\")\n",
        "        # except Exception as e2:\n",
        "        #     print(f\"Error loading MBPP dataset from alternative URL {mbpp_parquet_url_alt}: {e2}\")\n",
        "        print(\"Please ensure you have `pyarrow` and `fsspec` with `hf-transfer` installed if loading directly from hf:// URIs.\")\n",
        "        print(\"In Colab, you might need: !pip install -q pyarrow fsspec hf-transfer\")\n",
        "        return None\n",
        "\n",
        "    data_list = []\n",
        "    for index, item in mbpp_df.iterrows():\n",
        "        function_id = item.get(\"task_id\")\n",
        "        # The 'text' column contains the prompt/description\n",
        "        prompt_text = item.get(\"text\")\n",
        "        # The 'code' column contains the function code\n",
        "        function_code = item.get(\"code\")\n",
        "        # 'test_list' is a list of assertion strings\n",
        "        test_code_list = item.get(\"test_list\", [])\n",
        "        # 'test_setup_code' is any setup code needed for tests\n",
        "        test_setup_code = item.get(\"test_setup_code\", \"\")\n",
        "        entry_point = None\n",
        "\n",
        "        if isinstance(function_code, str) and function_code.strip():\n",
        "            try:\n",
        "                parsed_code = ast.parse(function_code)\n",
        "                for node in parsed_code.body:\n",
        "                    if isinstance(node, ast.FunctionDef):\n",
        "                        entry_point = node.name\n",
        "                        break\n",
        "            except SyntaxError:\n",
        "                # print(f\"SyntaxError parsing function code for entry point: {function_id}\")\n",
        "                pass\n",
        "\n",
        "        # Combine test_setup_code and test_list to form the full test code string\n",
        "        full_test_code_str = str(test_setup_code) if pd.notna(test_setup_code) else \"\"\n",
        "        if isinstance(test_code_list, list) and test_code_list:\n",
        "            full_test_code_str += (\"\\n\" if full_test_code_str else \"\") + \"\\n\".join(test_code_list)\n",
        "\n",
        "        assertions = extract_assertions_from_test(full_test_code_str)\n",
        "\n",
        "        data_list.append({\n",
        "            \"dataset_source\": \"mbpp\",\n",
        "            \"function_id\": str(function_id), # Ensure ID is string for consistency\n",
        "            \"prompt\": str(prompt_text) if pd.notna(prompt_text) else \"\",\n",
        "            \"function_code\": str(function_code) if pd.notna(function_code) else \"\",\n",
        "            \"test_code\": full_test_code_str,\n",
        "            \"assertions\": json.dumps(assertions), # Store assertions as a JSON string\n",
        "            \"assertions_count\": len(assertions),\n",
        "            \"entry_point\": entry_point\n",
        "        })\n",
        "\n",
        "    if not data_list:\n",
        "        print(\"No data extracted from MBPP dataset after processing.\")\n",
        "        return None\n",
        "\n",
        "    processed_df = pd.DataFrame(data_list)\n",
        "\n",
        "    # Ensure output directory exists\n",
        "    output_dir = os.path.dirname(output_csv_path)\n",
        "    if output_dir and not os.path.exists(output_dir):\n",
        "        try:\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "            print(f\"Created directory: {output_dir}\")\n",
        "        except OSError as e:\n",
        "            print(f\"Error creating directory {output_dir}: {e}. Attempting to save to current directory.\")\n",
        "            # Fallback to saving in the current directory if dir creation fails\n",
        "            output_csv_path = os.path.basename(output_csv_path)\n",
        "\n",
        "    try:\n",
        "        processed_df.to_csv(output_csv_path, index=False)\n",
        "        print(f\"MBPP dataset processed and saved to {output_csv_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving MBPP processed data to CSV at {output_csv_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "    return processed_df\n"
      ],
      "metadata": {
        "id": "-8lPVOjGbOAb"
      },
      "id": "-8lPVOjGbOAb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9KzR6UzTR4b"
      },
      "outputs": [],
      "source": [
        "# Run MBPP Data Collection and Processing\n",
        "print(\"Running MBPP Data Collection and Processing for Colab...\")\n",
        "\n",
        "mbpp_csv_path = \"/content/drive/MyDrive/Colab Notebooks/Assertion-Accuracy-Research/Data/mbpp_dataset.csv\"\n",
        "\n",
        "mbpp_df = load_and_process_mbpp(mbpp_csv_path)\n",
        "\n",
        "if mbpp_df is not None:\n",
        "    print(\"\\nMBPP Processed Dataset Statistics:\")\n",
        "    print(f\"Total number of entries: {len(mbpp_df)}\")\n",
        "    print(f\"Total assertions extracted: {mbpp_df['assertions_count'].sum()}\")\n",
        "    print(\"\\nMBPP Processed Dataset Preview (first 5 rows):\")\n",
        "    from IPython.display import display # For Colab\n",
        "    display(mbpp_df.head())\n",
        "else:\n",
        "    print(\"Failed to load and process the MBPP dataset.\")"
      ],
      "id": "m9KzR6UzTR4b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScSn4gd8TR4c"
      },
      "source": [
        "### 2.4 Combine and Finalize Datasets\n"
      ],
      "id": "ScSn4gd8TR4c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-5xOXa5TR4c"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display # For Colab\n",
        "\n",
        "\n",
        "print(\"\\n--- Combining Datasets ---\")\n",
        "\n",
        "# Adding a check here for robustness in case this cell is run out of order or BASE_DRIVE_PATH is not set.\n",
        "if \"BASE_DRIVE_PATH\" not in locals() and \"BASE_DRIVE_PATH\" not in globals():\n",
        "    print(\"Error: BASE_DRIVE_PATH is not defined. Please ensure it's set in a previous cell.\")\n",
        "    # Attempt to define a default if running in Colab, otherwise this will fail if not in Colab\n",
        "    try:\n",
        "        if \"google.colab\" in str(get_ipython()):\n",
        "            BASE_DRIVE_PATH = \"/content/drive/MyDrive/Colab Notebooks/Assertion-Accuracy-Research/Data\"\n",
        "            print(f\"Attempting to use default BASE_DRIVE_PATH: {BASE_DRIVE_PATH}\")\n",
        "            os.makedirs(BASE_DRIVE_PATH, exist_ok=True) # Ensure it exists\n",
        "        else:\n",
        "            raise NameError(\"BASE_DRIVE_PATH not defined and not in Colab to set default.\")\n",
        "    except NameError:\n",
        "        print(\"Cannot determine BASE_DRIVE_PATH. Please define it manually.\")\n",
        "        # Exit or raise error if BASE_DRIVE_PATH is critical and not found\n",
        "        raise ValueError(\"BASE_DRIVE_PATH must be defined to proceed with combining datasets.\")\n",
        "\n",
        "github_csv_path = os.path.join(BASE_DRIVE_PATH, \"github_dataset.csv\")\n",
        "humaneval_csv_path = os.path.join(BASE_DRIVE_PATH, \"humanevalplus_dataset.csv\")\n",
        "mbpp_csv_path = os.path.join(BASE_DRIVE_PATH, \"mbpp_dataset.csv\")\n",
        "combined_dataset_path = os.path.join(BASE_DRIVE_PATH, \"combined_dataset.csv\")\n",
        "\n",
        "all_dfs = []\n",
        "\n",
        "print(\"Note: GitHub data combination requires careful schema alignment and transformation (e.g., exploding lists of functions/tests per file into one function per row). This simplified version will focus on HumanEval+ and MBPP for the combined dataset if GitHub data is not pre-transformed.\")\n",
        "df_github_transformed = None # Placeholder for transformed GitHub data\n",
        "\n",
        "if os.path.exists(github_csv_path):\n",
        "    try:\n",
        "        df_github_raw = pd.read_csv(github_csv_path)\n",
        "        print(f\"Loaded raw GitHub data with {len(df_github_raw)} file entries. This data needs transformation to be combined effectively.\")\n",
        "        # Placeholder for transformation logic. For the purpose of this script, we will not combine it directly\n",
        "        # unless it's already in a compatible one-function-per-row format with necessary columns.\n",
        "        # Example columns needed: 'dataset_source', 'function_id', 'function_code', 'test_code', 'assertions', 'entry_point'\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or initially processing GitHub CSV: {e}\")\n",
        "else:\n",
        "    print(f\"GitHub dataset not found at {github_csv_path}. Skipping its inclusion in combined dataset for now.\")\n",
        "\n",
        "if os.path.exists(humaneval_csv_path):\n",
        "    try:\n",
        "        df_humaneval = pd.read_csv(humaneval_csv_path)\n",
        "        # Ensure required columns exist\n",
        "        required_cols_he = ['dataset_source', 'function_id', 'function_code', 'test_code', 'assertions', 'entry_point']\n",
        "        if all(col in df_humaneval.columns for col in required_cols_he):\n",
        "            df_humaneval = df_humaneval[required_cols_he].copy()\n",
        "            all_dfs.append(df_humaneval)\n",
        "            print(f\"Loaded HumanEval+ data: {len(df_humaneval)} entries.\")\n",
        "        else:\n",
        "            print(f\"HumanEval+ CSV at {humaneval_csv_path} is missing one or more required columns: {required_cols_he}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading HumanEval+ CSV from {humaneval_csv_path}: {e}\")\n",
        "else:\n",
        "    print(f\"HumanEval+ dataset not found at {humaneval_csv_path}. Skipping.\")\n",
        "\n",
        "if os.path.exists(mbpp_csv_path):\n",
        "    try:\n",
        "        df_mbpp = pd.read_csv(mbpp_csv_path)\n",
        "        required_cols_mbpp = ['dataset_source', 'function_id', 'function_code', 'test_code', 'assertions', 'entry_point']\n",
        "        if all(col in df_mbpp.columns for col in required_cols_mbpp):\n",
        "            df_mbpp = df_mbpp[required_cols_mbpp].copy()\n",
        "            all_dfs.append(df_mbpp)\n",
        "            print(f\"Loaded MBPP data: {len(df_mbpp)} entries.\")\n",
        "        else:\n",
        "            print(f\"MBPP CSV at {mbpp_csv_path} is missing one or more required columns: {required_cols_mbpp}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading MBPP CSV from {mbpp_csv_path}: {e}\")\n",
        "else:\n",
        "    print(f\"MBPP dataset not found at {mbpp_csv_path}. Skipping.\")\n",
        "\n",
        "if all_dfs:\n",
        "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
        "    combined_df.dropna(subset=['function_code'], inplace=True)\n",
        "    combined_df['function_code'] = combined_df['function_code'].astype(str)\n",
        "    combined_df['function_id'] = combined_df['function_id'].astype(str)\n",
        "    combined_df['dataset_source'] = combined_df['dataset_source'].astype(str)\n",
        "\n",
        "    combined_df['unique_id'] = combined_df['dataset_source'] + '_' + combined_df['function_id']\n",
        "\n",
        "    combined_df.drop_duplicates(subset=['function_code'], keep='first', inplace=True)\n",
        "    combined_df.drop_duplicates(subset=['unique_id'], keep='first', inplace=True) # Also ensure unique_id is unique\n",
        "\n",
        "    combined_df.to_csv(combined_dataset_path, index=False)\n",
        "    print(f\"\\nCombined dataset (from available sources) saved to {combined_dataset_path} with {len(combined_df)} unique functions.\")\n",
        "    print(\"\\nCombined Dataset Preview:\")\n",
        "    display(combined_df.head())\n",
        "    print(\"\\nCombined Dataset Info:\")\n",
        "    combined_df.info()\n",
        "else:\n",
        "    print(\"No datasets were loaded to combine. Ensure HumanEval+ and/or MBPP CSVs exist, are readable, and contain required columns.\")\n",
        "    # Create an empty DataFrame with expected columns to prevent downstream errors if combined_df is used\n",
        "    expected_cols = ['dataset_source', 'function_id', 'function_code', 'test_code', 'assertions', 'entry_point', 'unique_id']\n",
        "    combined_df = pd.DataFrame(columns=expected_cols)\n",
        "    # Save the empty df so the path exists\n",
        "    combined_df.to_csv(combined_dataset_path, index=False)\n",
        "    print(f\"Empty combined dataset template saved to {combined_dataset_path}\")\n"
      ],
      "id": "C-5xOXa5TR4c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqrCKeV0TR4d"
      },
      "source": [
        "## 3. LLM Setup and Test Generation\n",
        "\n",
        "This section focuses on setting up the DeepSeek and Microsoft Phi LLMs (downloadable models) and using them to generate test cases for the functions in our combined dataset.\n"
      ],
      "id": "RqrCKeV0TR4d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SwAGZu6OTR4d"
      },
      "outputs": [],
      "source": [
        "# It handles LLM setup, test generation, and baseline evaluation.\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import ast\n",
        "import time\n",
        "import re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "from IPython.display import display, Markdown # For Colab\n",
        "\n",
        "# --- LLM Loading Functions ---\n",
        "def load_llm_model(model_name, model_id, task=\"text-generation\", use_quantization=False):\n",
        "    \"\"\"Loads a specified LLM from Hugging Face with optional quantization.\"\"\"\n",
        "    print(f\"Loading LLM: {model_name} ({model_id})...\")\n",
        "    quantization_config = None\n",
        "    if use_quantization and torch.cuda.is_available():\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "        )\n",
        "        print(\"Applying 4-bit quantization.\")\n",
        "\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "        model_args = {\"trust_remote_code\": True}\n",
        "        if quantization_config:\n",
        "            model_args[\"quantization_config\"] = quantization_config\n",
        "            model_args[\"device_map\"] = \"auto\" # device_map auto is good for quantized models\n",
        "        else:\n",
        "            # For non-quantized, device_map=\"auto\" might put it on CPU if VRAM is low.\n",
        "            # Explicitly try to use CUDA if available and not quantizing, otherwise auto.\n",
        "            model_args[\"device_map\"] = \"cuda\" if torch.cuda.is_available() else \"auto\"\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_id, **model_args)\n",
        "\n",
        "        if tokenizer.pad_token_id is None:\n",
        "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "            model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "        # Determine torch_dtype based on availability and quantization\n",
        "        dtype_to_use = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
        "        if quantization_config: # Quantized models often handle dtype internally or prefer bfloat16 for compute\n",
        "             dtype_to_use = torch.bfloat16 # As specified in bnb_4bit_compute_dtype\n",
        "\n",
        "        pipe = pipeline(task, model=model, tokenizer=tokenizer, torch_dtype=dtype_to_use if not quantization_config else None)\n",
        "        print(f\"{model_name} loaded successfully on device: {model.device}\")\n",
        "        return pipe\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {model_name} ({model_id}): {e}\")\n",
        "        print(\"Ensure you have accepted any necessary terms on Hugging Face Hub for this model and have internet access.\")\n",
        "        print(\"If using quantization, ensure CUDA is available and drivers are up to date.\")\n",
        "        return None\n",
        "\n",
        "# --- Test Generation Function ---\n",
        "def generate_tests_with_llm(llm_pipeline, function_code, function_name, prompt_template, max_new_tokens=512, num_return_sequences=1):\n",
        "    \"\"\"Generates test code for a given function using the loaded LLM pipeline.\"\"\"\n",
        "    full_prompt = prompt_template.format(function_code=function_code, function_name=function_name)\n",
        "    try:\n",
        "        model_identifier = llm_pipeline.model.config._name_or_path if hasattr(llm_pipeline.model, \"config\") and hasattr(llm_pipeline.model.config, \"_name_or_path\") else \"Unknown LLM\"\n",
        "        print(f\"Generating tests for {function_name} using {model_identifier}...\")\n",
        "        sequences = llm_pipeline(\n",
        "            full_prompt,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_return_sequences=num_return_sequences,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            pad_token_id=llm_pipeline.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        generated_tests = []\n",
        "        for seq in sequences:\n",
        "            generated_code = seq[\"generated_text\"]\n",
        "            code_start_marker = \"```python\\n\"\n",
        "            code_end_marker = \"\\n```\"\n",
        "\n",
        "            # Try to find the last occurrence of the code block\n",
        "            start_index = generated_code.rfind(code_start_marker)\n",
        "            if start_index != -1:\n",
        "                start_index += len(code_start_marker)\n",
        "                end_index = generated_code.find(code_end_marker, start_index)\n",
        "                if end_index != -1:\n",
        "                    extracted_code = generated_code[start_index:end_index]\n",
        "                else: # If no end marker after the last start marker, take rest of string\n",
        "                    extracted_code = generated_code[start_index:]\n",
        "                generated_tests.append(extracted_code.strip())\n",
        "            else:\n",
        "                # Fallback: if no ```python marker, try to get text after the prompt\n",
        "                prompt_end_index = len(full_prompt)\n",
        "                if len(generated_code) > prompt_end_index:\n",
        "                     extracted_code = generated_code[prompt_end_index:].strip()\n",
        "                     if not extracted_code.startswith((\"def test_\", \"import\", \"assert\")):\n",
        "                         print(f\"Warning: Generated test for {function_name} (fallback extraction) might not be well-formed code. Raw output: {extracted_code[:200]}...\")\n",
        "                     generated_tests.append(extracted_code)\n",
        "                else:\n",
        "                    generated_tests.append(generated_code.strip())\n",
        "\n",
        "        print(f\"Successfully generated {len(generated_tests)} test sequence(s) for {function_name}.\")\n",
        "        return generated_tests[0] if generated_tests and num_return_sequences == 1 else generated_tests\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating tests for {function_name} with {model_identifier}: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Assertion Extraction and Basic Evaluation ---\n",
        "def extract_assertions_from_code(code_string):\n",
        "    assertions = []\n",
        "    if not isinstance(code_string, str):\n",
        "        return assertions\n",
        "    try:\n",
        "        tree = ast.parse(code_string)\n",
        "        for node in ast.walk(tree):\n",
        "            if isinstance(node, ast.Assert):\n",
        "                try:\n",
        "                    assertions.append(ast.unparse(node.test).strip())\n",
        "                except AttributeError:\n",
        "                    assertions.append(f\"Assertion involving node type: {type(node.test).__name__}\")\n",
        "    except SyntaxError:\n",
        "        pass\n",
        "    return assertions\n",
        "\n",
        "def baseline_evaluate_generated_tests(function_id, function_code, entry_point, generated_test_code, llm_model_name):\n",
        "    evaluation = {\n",
        "        \"unique_id\": function_id,\n",
        "        \"llm_model\": llm_model_name,\n",
        "        \"generated_test_code\": generated_test_code,\n",
        "        \"syntactic_validity\": False,\n",
        "        \"extracted_assertions\": [],\n",
        "        \"num_assertions\": 0,\n",
        "        \"execution_passes\": None,\n",
        "        \"execution_error\": None\n",
        "    }\n",
        "    if not generated_test_code or not isinstance(generated_test_code, str) or not generated_test_code.strip():\n",
        "        evaluation[\"execution_error\"] = \"Empty or invalid generated test code.\"\n",
        "        return evaluation\n",
        "\n",
        "    try:\n",
        "        ast.parse(generated_test_code)\n",
        "        evaluation[\"syntactic_validity\"] = True\n",
        "    except SyntaxError as e:\n",
        "        evaluation[\"execution_error\"] = f\"SyntaxError: {e}\"\n",
        "        return evaluation\n",
        "\n",
        "    evaluation[\"extracted_assertions\"] = extract_assertions_from_code(generated_test_code)\n",
        "    evaluation[\"num_assertions\"] = len(evaluation[\"extracted_assertions\"])\n",
        "\n",
        "    if evaluation[\"syntactic_validity\"] and entry_point and isinstance(function_code, str) and function_code.strip():\n",
        "        # Sanitize names for file operations\n",
        "        safe_function_id = re.sub(r\"[^a-zA-Z0-9_]\", \"\", str(function_id))\n",
        "        safe_llm_name = re.sub(r\"[^a-zA-Z0-9_]\", \"\", llm_model_name)\n",
        "        temp_module_name = f\"temp_test_module_{safe_function_id}_{safe_llm_name}\"\n",
        "\n",
        "        full_code_to_execute = function_code + \"\\n\\n\" + generated_test_code\n",
        "\n",
        "        if not re.search(r\"if __name__ == .__main__.:\", generated_test_code, re.IGNORECASE) and \"pytest\" not in generated_test_code.lower():\n",
        "            test_function_calls = []\n",
        "            try:\n",
        "                parsed_tests = ast.parse(generated_test_code)\n",
        "                for node in parsed_tests.body:\n",
        "                    if isinstance(node, ast.FunctionDef) and node.name.startswith(\"test\"):\n",
        "                        test_function_calls.append(f\"{node.name}()\")\n",
        "                if test_function_calls:\n",
        "                    full_code_to_execute += \"\\n\\n# Auto-added test runner\\n\" + \"\\n\".join(test_function_calls)\n",
        "            except SyntaxError:\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "            with open(f\"{temp_module_name}.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(full_code_to_execute)\n",
        "\n",
        "            import subprocess\n",
        "            process = subprocess.run([\"python\", f\"{temp_module_name}.py\"], capture_output=True, text=True, timeout=20, encoding=\"utf-8\", errors=\"replace\")\n",
        "\n",
        "            if process.returncode == 0:\n",
        "                evaluation[\"execution_passes\"] = True\n",
        "            else:\n",
        "                evaluation[\"execution_passes\"] = False\n",
        "                evaluation[\"execution_error\"] = process.stderr[:1000]\n",
        "        except FileNotFoundError:\n",
        "            evaluation[\"execution_error\"] = \"Python executable not found for subprocess execution.\"\n",
        "            evaluation[\"execution_passes\"] = False\n",
        "        except subprocess.TimeoutExpired:\n",
        "            evaluation[\"execution_error\"] = \"Test execution timed out.\"\n",
        "            evaluation[\"execution_passes\"] = False\n",
        "        except Exception as e:\n",
        "            evaluation[\"execution_error\"] = f\"Runtime error: {str(e)[:500]}\"\n",
        "            evaluation[\"execution_passes\"] = False\n",
        "        finally:\n",
        "            if os.path.exists(f\"{temp_module_name}.py\"):\n",
        "                os.remove(f\"{temp_module_name}.py\")\n",
        "    elif not isinstance(function_code, str) or not function_code.strip():\n",
        "        evaluation[\"execution_error\"] = \"Original function code is missing or empty.\"\n",
        "    elif not entry_point:\n",
        "        evaluation[\"execution_error\"] = \"Function entry point is missing.\"\n",
        "\n",
        "    return evaluation\n",
        "\n",
        "# --- Main Processing Function ---\n",
        "def run_llm_test_generation_and_evaluation(BASE_DRIVE_PATH):\n",
        "    print(\"\\n--- Starting LLM Test Generation and Baseline Evaluation ---\")\n",
        "\n",
        "    llm_models_to_use = {\n",
        "        \"DeepSeek-Coder-1.3B-instruct\": {\"id\": \"deepseek-ai/deepseek-coder-1.3b-instruct\", \"quant\": True},\n",
        "        \"Phi-2\": {\"id\": \"microsoft/phi-2\", \"quant\": True}\n",
        "    }\n",
        "\n",
        "    loaded_llms = {}\n",
        "    for name, config in llm_models_to_use.items():\n",
        "        llm_pipe = load_llm_model(name, config[\"id\"], use_quantization=config[\"quant\"])\n",
        "        if llm_pipe:\n",
        "            loaded_llms[name] = llm_pipe\n",
        "\n",
        "    if not loaded_llms:\n",
        "        print(\"No LLMs were loaded successfully. Aborting test generation.\")\n",
        "        return\n",
        "\n",
        "    combined_dataset_path = os.path.join(BASE_DRIVE_PATH, \"combined_dataset.csv\")\n",
        "    if not os.path.exists(combined_dataset_path):\n",
        "        print(f\"Combined dataset not found at {combined_dataset_path}. Please run data collection first.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        combined_df = pd.read_csv(combined_dataset_path)\n",
        "        print(f\"Loaded combined dataset with {len(combined_df)} functions.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading combined_dataset.csv: {e}\")\n",
        "        return\n",
        "\n",
        "    prompt_template = \"\"\"You are an expert Python programmer. Given the following Python function, please generate a comprehensive suite of pytest-style unit tests. Ensure your tests cover various edge cases and typical scenarios. The tests should include clear assertion statements. Do not include the original function in your response, only the test code. Wrap the test code in ```python ... ```.\\n\\nFunction Code:\\n```python\\n{function_code}\\n```\\n\\nFunction Name: `{function_name}`\\n\\nPlease provide the pytest unit tests below:\\n```python\\n# Your pytest unit tests here\\n\"\"\"\n",
        "\n",
        "    all_generated_tests_data = []\n",
        "    baseline_evaluation_results = []\n",
        "\n",
        "    # Consider processing a subset for faster runs in Colab during development\n",
        "    # num_functions_to_process = min(len(combined_df), 10)\n",
        "    # print(f\"Processing a subset of {num_functions_to_process} functions for test generation.\")\n",
        "    # combined_df_subset = combined_df.sample(n=num_functions_to_process, random_state=42) if len(combined_df) > num_functions_to_process else combined_df\n",
        "    combined_df_subset = combined_df # Process all for full run\n",
        "\n",
        "    for index, row in combined_df_subset.iterrows():\n",
        "        unique_id = row[\"unique_id\"]\n",
        "        function_code = str(row[\"function_code\"])\n",
        "        entry_point = str(row.get(\"entry_point\", \"\"))\n",
        "\n",
        "        if not function_code.strip() or not entry_point.strip():\n",
        "            print(f\"Skipping function {unique_id} due to missing code or entry point.\")\n",
        "            baseline_evaluation_results.append({\n",
        "                \"unique_id\": unique_id, \"llm_model\": \"N/A\", \"generated_test_code\": None,\n",
        "                \"syntactic_validity\": False, \"extracted_assertions\": [], \"num_assertions\": 0,\n",
        "                \"execution_passes\": None, \"execution_error\": \"Missing function code or entry point\"\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nProcessing function: {unique_id} ({entry_point})\")\n",
        "        for llm_name, llm_pipe in loaded_llms.items():\n",
        "            generated_test_code = generate_tests_with_llm(llm_pipe, function_code, entry_point, prompt_template)\n",
        "\n",
        "            all_generated_tests_data.append({\n",
        "                \"unique_id\": unique_id,\n",
        "                \"function_code\": function_code,\n",
        "                \"entry_point\": entry_point,\n",
        "                \"llm_model\": llm_name,\n",
        "                \"generated_test_code\": generated_test_code if generated_test_code else \"\"\n",
        "            })\n",
        "\n",
        "            evaluation_metrics = baseline_evaluate_generated_tests(unique_id, function_code, entry_point, generated_test_code, llm_name)\n",
        "            baseline_evaluation_results.append(evaluation_metrics)\n",
        "            time.sleep(0.5)\n",
        "\n",
        "    output_dir = os.path.join(BASE_DRIVE_PATH, \"llm_generated_tests\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    generated_tests_df = pd.DataFrame(all_generated_tests_data)\n",
        "    generated_tests_path = os.path.join(output_dir, \"llm_generated_tests_output.csv\")\n",
        "    generated_tests_df.to_csv(generated_tests_path, index=False)\n",
        "    print(f\"\\nLLM generated tests saved to {generated_tests_path}\")\n",
        "    if not generated_tests_df.empty: display(generated_tests_df.head())\n",
        "\n",
        "    baseline_eval_df = pd.DataFrame(baseline_evaluation_results)\n",
        "    baseline_eval_path = os.path.join(output_dir, \"baseline_evaluation_summary.csv\")\n",
        "    baseline_eval_df.to_csv(baseline_eval_path, index=False)\n",
        "    print(f\"Baseline evaluation summary saved to {baseline_eval_path}\")\n",
        "    if not baseline_eval_df.empty: display(baseline_eval_df.head())\n",
        "\n",
        "    print(\"\\n--- LLM Test Generation and Baseline Evaluation Completed ---\")"
      ],
      "id": "SwAGZu6OTR4d"
    },
    {
      "cell_type": "code",
      "source": [
        "run_llm_test_generation_and_evaluation(BASE_DRIVE_PATH)"
      ],
      "metadata": {
        "id": "7k9_1bxWbEr5"
      },
      "id": "7k9_1bxWbEr5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjDQXa8XTR4d"
      },
      "source": [
        "## 4. Assertion Extraction and Baseline Evaluation\n",
        "\n",
        "After generating tests, we extract the assertions and perform a baseline evaluation to understand their initial quality (e.g., syntactic validity, number of assertions).The `baseline_evaluate_generated_tests` function from the `llm_test_gen_eval.py` script handles this.\n"
      ],
      "id": "WjDQXa8XTR4d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEYJPfTUTR4e"
      },
      "source": [
        "## 5. Mutation Testing\n",
        "\n",
        "Mutation testing is used to assess the fault-detection capability of the generated assertions. We generate mutants of the original functions and check if the assertions can \"kill\" (detect) these mutants.\n"
      ],
      "id": "NEYJPfTUTR4e"
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of Mutation Testing\n",
        "import ast\n",
        "import random\n",
        "import re\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "from IPython.display import display\n",
        "\n",
        "# --- Enhanced Mutation Operators ---\n",
        "def mutate_comparison_operator(node):\n",
        "    \"\"\"Mutates comparison operators (e.g., > to <=, == to !=).\"\"\"\n",
        "    if isinstance(node, ast.Compare):\n",
        "        original_op = type(node.ops[0])\n",
        "        mutated = False\n",
        "        # More robust mutations: > to <=, < to >=, == to !=, etc.\n",
        "        if original_op == ast.Gt: node.ops[0] = ast.LtE(); mutated = True\n",
        "        elif original_op == ast.Lt: node.ops[0] = ast.GtE(); mutated = True\n",
        "        elif original_op == ast.GtE: node.ops[0] = ast.Lt(); mutated = True\n",
        "        elif original_op == ast.LtE: node.ops[0] = ast.Gt(); mutated = True\n",
        "        elif original_op == ast.Eq: node.ops[0] = ast.NotEq(); mutated = True\n",
        "        elif original_op == ast.NotEq: node.ops[0] = ast.Eq(); mutated = True\n",
        "        elif original_op == ast.Is: node.ops[0] = ast.IsNot(); mutated = True\n",
        "        elif original_op == ast.IsNot: node.ops[0] = ast.Is(); mutated = True\n",
        "        return mutated\n",
        "    return False\n",
        "\n",
        "def mutate_arithmetic_operator(node):\n",
        "    \"\"\"Mutates arithmetic operators (e.g., + to -, * to /).\"\"\"\n",
        "    if isinstance(node, ast.BinOp):\n",
        "        original_op = type(node.op)\n",
        "        mutated = False\n",
        "        if original_op == ast.Add: node.op = ast.Sub(); mutated = True\n",
        "        elif original_op == ast.Sub: node.op = ast.Add(); mutated = True\n",
        "        elif original_op == ast.Mult: node.op = ast.Div(); mutated = True\n",
        "        elif original_op == ast.Div: node.op = ast.Mult(); mutated = True\n",
        "        return mutated\n",
        "    return False\n",
        "\n",
        "def mutate_logical_operator(node):\n",
        "    \"\"\"Mutates logical operators (e.g., and to or, or to and).\"\"\"\n",
        "    if isinstance(node, ast.BoolOp):\n",
        "        original_op = type(node.op)\n",
        "        mutated = False\n",
        "        if original_op == ast.And: node.op = ast.Or(); mutated = True\n",
        "        elif original_op == ast.Or: node.op = ast.And(); mutated = True\n",
        "        return mutated\n",
        "    elif isinstance(node, ast.UnaryOp) and isinstance(node.op, ast.Not):\n",
        "        # Remove the Not operation by replacing with the operand\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def mutate_statement_deletion(node):\n",
        "    \"\"\"Modifies or effectively deletes statements like returns or assignments.\"\"\"\n",
        "    if isinstance(node, ast.Return) and hasattr(node, 'value') and node.value:\n",
        "        # Modify return values\n",
        "        if isinstance(node.value, ast.Constant):\n",
        "            if isinstance(node.value.value, bool):\n",
        "                node.value.value = not node.value.value\n",
        "                return True\n",
        "            elif isinstance(node.value.value, (int, float)):\n",
        "                node.value.value = 0  # Replace with default/zero value\n",
        "                return True\n",
        "            elif isinstance(node.value.value, str):\n",
        "                node.value.value = \"\"  # Replace with empty string\n",
        "                return True\n",
        "    elif isinstance(node, ast.Assign):\n",
        "        # Modify assignments with constants\n",
        "        if len(node.targets) == 1 and isinstance(node.value, ast.Constant):\n",
        "            if isinstance(node.value.value, (int, float)):\n",
        "                node.value.value = 0\n",
        "                return True\n",
        "            elif isinstance(node.value.value, bool):\n",
        "                node.value.value = not node.value.value\n",
        "                return True\n",
        "            elif isinstance(node.value.value, str):\n",
        "                node.value.value = \"\"\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "def mutate_boundary_value(node):\n",
        "    \"\"\"Mutates constant numeric values with boundary-focused mutations.\"\"\"\n",
        "    if isinstance(node, ast.Constant) and isinstance(node.value, (int, float)):\n",
        "        original_value = node.value\n",
        "\n",
        "        # Special cases for common boundaries\n",
        "        if original_value == 1:\n",
        "            node.value = 0 if random.random() < 0.5 else 2\n",
        "        elif original_value == 0:\n",
        "            node.value = -1 if random.random() < 0.5 else 1\n",
        "        elif original_value > 0:\n",
        "            # For positive values, consider boundary cases\n",
        "            choices = [-original_value, original_value-1, original_value+1, 0]\n",
        "            node.value = random.choice(choices)\n",
        "        else:  # original_value < 0\n",
        "            choices = [-original_value, original_value-1, original_value+1, 0]\n",
        "            node.value = random.choice(choices)\n",
        "\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Combine all mutation operators\n",
        "MUTATION_OPERATORS = [\n",
        "    mutate_comparison_operator,\n",
        "    mutate_arithmetic_operator,\n",
        "    mutate_logical_operator,\n",
        "    mutate_statement_deletion,\n",
        "    mutate_boundary_value\n",
        "]\n",
        "\n",
        "# --- Mutation Generation Process ---\n",
        "def generate_mutants(original_code_str, num_mutants_per_function=5):\n",
        "    \"\"\"Generate mutants with improved systematic approach and filtering.\"\"\"\n",
        "    mutants = []\n",
        "    stats = {\n",
        "        \"requested\": num_mutants_per_function,\n",
        "        \"generated\": 0,\n",
        "        \"attempts\": 0,\n",
        "        \"equivalent_detected\": 0,\n",
        "        \"syntax_errors\": 0\n",
        "    }\n",
        "\n",
        "    if not original_code_str or not isinstance(original_code_str, str) or not original_code_str.strip():\n",
        "        return mutants, stats\n",
        "\n",
        "    try:\n",
        "        original_tree = ast.parse(original_code_str)\n",
        "    except SyntaxError:\n",
        "        stats[\"syntax_errors\"] += 1\n",
        "        return mutants, stats\n",
        "\n",
        "    generated_mutant_codes = set()\n",
        "    max_attempts = num_mutants_per_function * 15  # More attempts to find good mutants\n",
        "\n",
        "    while len(mutants) < num_mutants_per_function and stats[\"attempts\"] < max_attempts:\n",
        "        stats[\"attempts\"] += 1\n",
        "\n",
        "        mutant_tree = ast.parse(original_code_str)  # Re-parse for a fresh tree\n",
        "        nodes_in_mutant_tree = [n for n in ast.walk(mutant_tree)]\n",
        "        random.shuffle(nodes_in_mutant_tree)\n",
        "\n",
        "        mutation_applied = False\n",
        "        for node_to_mutate in nodes_in_mutant_tree:\n",
        "            # Try each operator in random order\n",
        "            shuffled_operators = random.sample(MUTATION_OPERATORS, len(MUTATION_OPERATORS))\n",
        "            for operator_func in shuffled_operators:\n",
        "                if operator_func(node_to_mutate):\n",
        "                    mutation_applied = True\n",
        "                    break\n",
        "            if mutation_applied:\n",
        "                break\n",
        "\n",
        "        if mutation_applied:\n",
        "            try:\n",
        "                mutant_code = ast.unparse(mutant_tree)\n",
        "\n",
        "                # Check if this is an equivalent mutant or duplicate\n",
        "                if mutant_code == original_code_str:\n",
        "                    stats[\"equivalent_detected\"] += 1\n",
        "                    continue\n",
        "\n",
        "                if mutant_code not in generated_mutant_codes:\n",
        "                    # Verify the mutated code is still valid Python\n",
        "                    ast.parse(mutant_code)\n",
        "                    mutants.append(mutant_code)\n",
        "                    generated_mutant_codes.add(mutant_code)\n",
        "                    stats[\"generated\"] += 1\n",
        "            except SyntaxError:\n",
        "                stats[\"syntax_errors\"] += 1\n",
        "            except Exception:\n",
        "                pass  # Other errors during generation\n",
        "\n",
        "    return mutants, stats\n",
        "\n",
        "# --- Test Execution against Mutants ---\n",
        "def run_tests_on_mutant(function_entry_point, mutant_code_str, test_code_str, original_function_code_str):\n",
        "    \"\"\"Execute tests against a mutated function and check if the tests kill the mutant.\"\"\"\n",
        "    if not test_code_str or not isinstance(test_code_str, str) or not test_code_str.strip() or \\\n",
        "       not mutant_code_str or not isinstance(mutant_code_str, str) or not mutant_code_str.strip():\n",
        "        return False\n",
        "\n",
        "    full_script_content = mutant_code_str + \"\\n\\n\" + test_code_str\n",
        "\n",
        "    if not re.search(r\"if __name__ == .__main__.:\", test_code_str, re.IGNORECASE) and \"pytest\" not in test_code_str.lower():\n",
        "        test_function_calls = []\n",
        "        try:\n",
        "            parsed_tests = ast.parse(test_code_str)\n",
        "            for node in parsed_tests.body:\n",
        "                if isinstance(node, ast.FunctionDef) and node.name.startswith(\"test\"):\n",
        "                    test_function_calls.append(f\"{node.name}()\")\n",
        "            if test_function_calls:\n",
        "                full_script_content += \"\\n\\n# Auto-added test runner\\n\" + \"\\n\".join(test_function_calls)\n",
        "        except SyntaxError:\n",
        "            pass\n",
        "\n",
        "    # Sanitize names for file operations\n",
        "    safe_entry_point = re.sub(r\"[^a-zA-Z0-9_]\", \"\", str(function_entry_point))\n",
        "    temp_script_path = f\"temp_mutant_test_runner_{safe_entry_point}.py\"\n",
        "\n",
        "    try:\n",
        "        with open(temp_script_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(full_script_content)\n",
        "\n",
        "        process = subprocess.run([sys.executable, temp_script_path],\n",
        "                                capture_output=True, text=True, timeout=20,\n",
        "                                encoding=\"utf-8\", errors=\"replace\")\n",
        "\n",
        "        # Mutant is killed if tests fail (non-zero return code)\n",
        "        return process.returncode != 0\n",
        "\n",
        "    except subprocess.TimeoutExpired:\n",
        "        return True  # Timeout indicates infinite loop - consider killed\n",
        "    except Exception:\n",
        "        return False  # Other errors in execution - consider survived\n",
        "    finally:\n",
        "        if os.path.exists(temp_script_path):\n",
        "            try:\n",
        "                os.remove(temp_script_path)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "# --- External Mutation Tool Integration ---\n",
        "def run_external_mutation_tool(function_file, test_file):\n",
        "    \"\"\"Run mutation testing using external tools like mutmut or MutPy.\"\"\"\n",
        "    results = {\"success\": False, \"tool\": None, \"score\": 0, \"details\": {}}\n",
        "\n",
        "    # Try mutmut first\n",
        "    try:\n",
        "        import subprocess\n",
        "        print(\"Attempting to run mutmut...\")\n",
        "\n",
        "        with open(function_file, \"w\") as f:\n",
        "            f.write(\"# Function file for mutation testing\")\n",
        "\n",
        "        with open(test_file, \"w\") as f:\n",
        "            f.write(\"# Test file for mutation testing\")\n",
        "\n",
        "        result = subprocess.run(\n",
        "            [\"mutmut\", \"run\", \"--paths-to-mutate\", function_file, \"--test-command\", f\"python -m pytest {test_file}\"],\n",
        "            capture_output=True, text=True, timeout=300\n",
        "        )\n",
        "\n",
        "        if result.returncode == 0 or \"mutants\" in result.stdout:\n",
        "            results[\"success\"] = True\n",
        "            results[\"tool\"] = \"mutmut\"\n",
        "            # Extract score from output\n",
        "            results[\"details\"][\"output\"] = result.stdout\n",
        "\n",
        "            # Very basic parsing - would need to be refined\n",
        "            if \"killed\" in result.stdout and \"survived\" in result.stdout:\n",
        "                killed_match = re.search(r\"(\\d+) killed\", result.stdout)\n",
        "                survived_match = re.search(r\"(\\d+) survived\", result.stdout)\n",
        "\n",
        "                if killed_match and survived_match:\n",
        "                    killed = int(killed_match.group(1))\n",
        "                    survived = int(survived_match.group(1))\n",
        "                    total = killed + survived\n",
        "\n",
        "                    if total > 0:\n",
        "                        results[\"score\"] = (killed / total) * 100\n",
        "\n",
        "            return results\n",
        "    except Exception as e:\n",
        "        print(f\"mutmut failed: {e}\")\n",
        "\n",
        "    # Fallback to MutPy if mutmut fails\n",
        "    try:\n",
        "        print(\"Attempting to run MutPy...\")\n",
        "        result = subprocess.run(\n",
        "            [\"mut.py\", \"--target\", function_file, \"--unit-test\", test_file],\n",
        "            capture_output=True, text=True, timeout=300\n",
        "        )\n",
        "\n",
        "        if result.returncode == 0 or \"score\" in result.stdout:\n",
        "            results[\"success\"] = True\n",
        "            results[\"tool\"] = \"MutPy\"\n",
        "            results[\"details\"][\"output\"] = result.stdout\n",
        "\n",
        "            # Extract score from output\n",
        "            score_match = re.search(r\"Mutation score.*?(\\d+\\.\\d+)%\", result.stdout)\n",
        "            if score_match:\n",
        "                results[\"score\"] = float(score_match.group(1))\n",
        "\n",
        "            return results\n",
        "    except Exception as e:\n",
        "        print(f\"MutPy failed: {e}\")\n",
        "\n",
        "    print(\"External mutation tools unavailable or failed.\")\n",
        "    return results\n",
        "\n",
        "# --- Main Mutation Testing Function ---\n",
        "def run_mutation_testing_for_llm_tests(BASE_DRIVE_PATH):\n",
        "    print(\"\\n--- Starting Enhanced Mutation Testing for LLM-Generated Tests ---\")\n",
        "\n",
        "    llm_tests_output_path = os.path.join(BASE_DRIVE_PATH, \"llm_generated_tests\", \"llm_generated_tests_output.csv\")\n",
        "    if not os.path.exists(llm_tests_output_path):\n",
        "        print(f\"LLM generated tests output not found at {llm_tests_output_path}. Run LLM test generation first.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        llm_tests_df = pd.read_csv(llm_tests_output_path)\n",
        "        print(f\"Loaded {len(llm_tests_df)} LLM-generated test instances.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading LLM generated tests: {e}\")\n",
        "        return\n",
        "\n",
        "    mutation_results = []\n",
        "    mutation_statistics = []\n",
        "    num_mutants_per_func = 7  # Increased from 5 to 7 for more comprehensive testing\n",
        "\n",
        "    # Process all instances or use a subset for testing\n",
        "    llm_tests_df_subset = llm_tests_df\n",
        "\n",
        "    for index, row in llm_tests_df_subset.iterrows():\n",
        "        unique_id = row[\"unique_id\"]\n",
        "        original_function_code = str(row[\"function_code\"])\n",
        "        llm_generated_test_code = str(row[\"generated_test_code\"])\n",
        "        llm_model_name = row[\"llm_model\"]\n",
        "        entry_point = str(row.get(\"entry_point\", \"\"))\n",
        "\n",
        "        if (pd.isna(original_function_code) or pd.isna(llm_generated_test_code) or\n",
        "                not original_function_code.strip() or not llm_generated_test_code.strip() or not entry_point.strip()):\n",
        "            mutation_results.append({\n",
        "                \"unique_id\": unique_id,\n",
        "                \"llm_model\": llm_model_name,\n",
        "                \"total_mutants_generated\": 0,\n",
        "                \"mutants_killed\": 0,\n",
        "                \"mutation_score\": 0.0,\n",
        "                \"error\": \"Missing function, test code, or entry point\"\n",
        "            })\n",
        "            mutation_statistics.append({\n",
        "                \"unique_id\": unique_id,\n",
        "                \"llm_model\": llm_model_name,\n",
        "                \"requested\": num_mutants_per_func,\n",
        "                \"generated\": 0,\n",
        "                \"attempts\": 0,\n",
        "                \"equivalent_detected\": 0,\n",
        "                \"syntax_errors\": 0\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nRunning mutation testing for function {unique_id} ({entry_point}), tests by {llm_model_name}...\")\n",
        "        mutants, stats = generate_mutants(original_function_code, num_mutants_per_function=num_mutants_per_func)\n",
        "        stats[\"unique_id\"] = unique_id\n",
        "        stats[\"llm_model\"] = llm_model_name\n",
        "        mutation_statistics.append(stats)\n",
        "\n",
        "        if not mutants:\n",
        "            print(f\"No valid mutants generated for {unique_id} after {stats['attempts']} attempts.\")\n",
        "            mutation_results.append({\n",
        "                \"unique_id\": unique_id,\n",
        "                \"llm_model\": llm_model_name,\n",
        "                \"total_mutants_generated\": 0,\n",
        "                \"mutants_killed\": 0,\n",
        "                \"mutation_score\": 0.0,\n",
        "                \"error\": f\"No valid mutants generated after {stats['attempts']} attempts\"\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        mutants_killed = 0\n",
        "        killed_details = []\n",
        "        survived_mutations = {}\n",
        "\n",
        "        print(f\"Generated {len(mutants)} valid mutants for {unique_id}. Testing each...\")\n",
        "\n",
        "        for i, mutant_code in enumerate(mutants):\n",
        "            killed = run_tests_on_mutant(entry_point, mutant_code, llm_generated_test_code, original_function_code)\n",
        "            if killed:\n",
        "                mutants_killed += 1\n",
        "                killed_details.append(i)\n",
        "            else:\n",
        "                # Track types of survived mutations\n",
        "                # This is a simple approach - in practice you'd want to analyze the mutation more precisely\n",
        "                if \"==\" in mutant_code and \"!=\" in original_function_code:\n",
        "                    survived_mutations[\"comparison_operators\"] = survived_mutations.get(\"comparison_operators\", 0) + 1\n",
        "                elif \"+\" in mutant_code and \"-\" in original_function_code:\n",
        "                    survived_mutations[\"arithmetic_operators\"] = survived_mutations.get(\"arithmetic_operators\", 0) + 1\n",
        "                elif \"and\" in mutant_code and \"or\" in original_function_code:\n",
        "                    survived_mutations[\"logical_operators\"] = survived_mutations.get(\"logical_operators\", 0) + 1\n",
        "                else:\n",
        "                    survived_mutations[\"other\"] = survived_mutations.get(\"other\", 0) + 1\n",
        "\n",
        "        mutation_score = (mutants_killed / len(mutants)) * 100 if mutants else 0.0\n",
        "        print(f\"Mutation testing for {unique_id} by {llm_model_name}: {mutants_killed}/{len(mutants)} killed. Score: {mutation_score:.2f}%\")\n",
        "\n",
        "        mutation_results.append({\n",
        "            \"unique_id\": unique_id,\n",
        "            \"llm_model\": llm_model_name,\n",
        "            \"original_function_code\": original_function_code,\n",
        "            \"llm_generated_test_code\": llm_generated_test_code,\n",
        "            \"total_mutants_generated\": len(mutants),\n",
        "            \"mutants_killed\": mutants_killed,\n",
        "            \"mutation_score\": mutation_score,\n",
        "            \"killed_mutant_indices\": str(killed_details),\n",
        "            \"survived_mutations\": str(survived_mutations),\n",
        "            \"error\": None\n",
        "        })\n",
        "\n",
        "        time.sleep(0.2)  # Small delay\n",
        "\n",
        "    # Save detailed results\n",
        "    mutation_results_df = pd.DataFrame(mutation_results)\n",
        "    mutation_stats_df = pd.DataFrame(mutation_statistics)\n",
        "\n",
        "    output_dir = os.path.join(BASE_DRIVE_PATH, \"mutation_testing_results\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    mutation_results_path = os.path.join(output_dir, \"mutation_testing_summary.csv\")\n",
        "    mutation_stats_path = os.path.join(output_dir, \"mutation_generation_statistics.csv\")\n",
        "\n",
        "    mutation_results_df.to_csv(mutation_results_path, index=False)\n",
        "    mutation_stats_df.to_csv(mutation_stats_path, index=False)\n",
        "\n",
        "    print(f\"\\nMutation testing results saved to {mutation_results_path}\")\n",
        "    print(f\"Mutation generation statistics saved to {mutation_stats_path}\")\n",
        "\n",
        "    if not mutation_results_df.empty:\n",
        "        display(mutation_results_df.head())\n",
        "\n",
        "        # Calculate and display summary statistics by LLM model\n",
        "        model_summary = mutation_results_df.groupby(\"llm_model\").agg({\n",
        "            \"total_mutants_generated\": \"sum\",\n",
        "            \"mutants_killed\": \"sum\",\n",
        "            \"mutation_score\": \"mean\"\n",
        "        }).reset_index()\n",
        "        model_summary[\"overall_kill_rate\"] = (\n",
        "            model_summary[\"mutants_killed\"] / model_summary[\"total_mutants_generated\"] * 100\n",
        "        ).round(2)\n",
        "\n",
        "        print(\"\\nMutation Testing Summary by LLM Model:\")\n",
        "        display(model_summary)\n",
        "\n",
        "    print(\"\\n--- Enhanced Mutation Testing for LLM-Generated Tests Completed ---\")\n",
        "\n"
      ],
      "metadata": {
        "id": "AZ-BqEVyqvIZ"
      },
      "id": "AZ-BqEVyqvIZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_mutation_testing_for_llm_tests(BASE_DRIVE_PATH)"
      ],
      "metadata": {
        "id": "gzAawTntrNvC"
      },
      "id": "gzAawTntrNvC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of Assertion Refinement Dataset Creation\n",
        "\n",
        "def analyze_assertions_for_refinement(BASE_DRIVE_PATH):\n",
        "    \"\"\"Analyze assertion quality and create a dataset for refinement.\"\"\"\n",
        "    print(\"\\n--- Creating Assertion Refinement Dataset ---\")\n",
        "\n",
        "    mutation_results_path = os.path.join(BASE_DRIVE_PATH, \"mutation_testing_results\", \"mutation_testing_summary.csv\")\n",
        "    baseline_eval_path = os.path.join(BASE_DRIVE_PATH, \"llm_generated_tests\", \"baseline_evaluation_summary.csv\")\n",
        "\n",
        "    if not os.path.exists(mutation_results_path) or not os.path.exists(baseline_eval_path):\n",
        "        print(\"Required data for assertion analysis not found. Run mutation testing first.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        mutation_df = pd.read_csv(mutation_results_path)\n",
        "        baseline_df = pd.read_csv(baseline_eval_path)\n",
        "\n",
        "        # Merge for comprehensive analysis\n",
        "        analysis_df = pd.merge(\n",
        "            baseline_df[['unique_id', 'llm_model', 'execution_passes', 'syntactic_validity', 'num_assertions', 'extracted_assertions']],\n",
        "            mutation_df[['unique_id', 'llm_model', 'mutation_score', 'mutants_killed', 'total_mutants_generated']],\n",
        "            on=['unique_id', 'llm_model'],\n",
        "            how='inner'\n",
        "        )\n",
        "\n",
        "        print(f\"Loaded data for {len(analysis_df)} test cases for assertion quality analysis.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or merging data: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Classify assertions based on quality\n",
        "    analysis_df['assertion_quality'] = 'unknown'\n",
        "\n",
        "    # Classification logic:\n",
        "    # 1. Strong assertions: High mutation score (kill most mutants)\n",
        "    # 2. Weak assertions: Low mutation score (kill few mutants)\n",
        "    # 3. Incorrect assertions: Fail on correct implementation\n",
        "\n",
        "    analysis_df.loc[analysis_df['mutation_score'] >= 80, 'assertion_quality'] = 'strong'\n",
        "    analysis_df.loc[(analysis_df['mutation_score'] < 80) & (analysis_df['mutation_score'] >= 30), 'assertion_quality'] = 'moderate'\n",
        "    analysis_df.loc[analysis_df['mutation_score'] < 30, 'assertion_quality'] = 'weak'\n",
        "    analysis_df.loc[analysis_df['execution_passes'] == False, 'assertion_quality'] = 'incorrect'\n",
        "\n",
        "    # Extract and analyze individual assertions\n",
        "    def extract_assertion_details(row):\n",
        "        try:\n",
        "            extracted = ast.literal_eval(row['extracted_assertions']) if isinstance(row['extracted_assertions'], str) else []\n",
        "            if not extracted or not isinstance(extracted, list):\n",
        "                return []\n",
        "\n",
        "            details = []\n",
        "            for assertion in extracted:\n",
        "                # Basic classification of assertion types\n",
        "                assertion_type = 'general'\n",
        "                if 'assert isinstance' in assertion:\n",
        "                    assertion_type = 'type_check'\n",
        "                elif 'assert len' in assertion:\n",
        "                    assertion_type = 'length_check'\n",
        "                elif '==' in assertion:\n",
        "                    assertion_type = 'equality'\n",
        "                elif 'is not None' in assertion:\n",
        "                    assertion_type = 'existence'\n",
        "                elif 'is None' in assertion:\n",
        "                    assertion_type = 'absence'\n",
        "                elif 'in ' in assertion:\n",
        "                    assertion_type = 'membership'\n",
        "                elif 'not in ' in assertion:\n",
        "                    assertion_type = 'exclusion'\n",
        "                elif '>' in assertion or '<' in assertion:\n",
        "                    assertion_type = 'comparison'\n",
        "\n",
        "                details.append({\n",
        "                    'text': assertion,\n",
        "                    'type': assertion_type,\n",
        "                    'quality': row['assertion_quality'],\n",
        "                    'mutation_score': row['mutation_score']\n",
        "                })\n",
        "            return details\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "    all_assertion_details = []\n",
        "    for _, row in analysis_df.iterrows():\n",
        "        details = extract_assertion_details(row)\n",
        "        for detail in details:\n",
        "            detail['unique_id'] = row['unique_id']\n",
        "            detail['llm_model'] = row['llm_model']\n",
        "            all_assertion_details.append(detail)\n",
        "\n",
        "    assertion_details_df = pd.DataFrame(all_assertion_details)\n",
        "\n",
        "    # Split into training, validation, and test sets\n",
        "    if len(assertion_details_df) > 0:\n",
        "        # Shuffle the DataFrame\n",
        "        assertion_details_df = assertion_details_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "        # Calculate split indices\n",
        "        train_end = int(len(assertion_details_df) * 0.7)\n",
        "        val_end = int(len(assertion_details_df) * 0.85)\n",
        "\n",
        "        # Split the DataFrame\n",
        "        train_df = assertion_details_df.iloc[:train_end].copy()\n",
        "        val_df = assertion_details_df.iloc[train_end:val_end].copy()\n",
        "        test_df = assertion_details_df.iloc[val_end:].copy()\n",
        "\n",
        "        # Save the splits\n",
        "        output_dir = os.path.join(BASE_DRIVE_PATH, \"assertion_refinement_dataset\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        train_df.to_csv(os.path.join(output_dir, \"train.csv\"), index=False)\n",
        "        val_df.to_csv(os.path.join(output_dir, \"validation.csv\"), index=False)\n",
        "        test_df.to_csv(os.path.join(output_dir, \"test.csv\"), index=False)\n",
        "\n",
        "        # Also save the full dataset\n",
        "        assertion_details_df.to_csv(os.path.join(output_dir, \"full_dataset.csv\"), index=False)\n",
        "\n",
        "        print(f\"Assertion refinement dataset created with {len(train_df)} training, {len(val_df)} validation, and {len(test_df)} test samples.\")\n",
        "        print(f\"Dataset saved to {output_dir}\")\n",
        "\n",
        "        # Summary statistics\n",
        "        print(\"\\nAssertion Quality Distribution:\")\n",
        "        quality_counts = analysis_df['assertion_quality'].value_counts()\n",
        "        for quality, count in quality_counts.items():\n",
        "            print(f\"  {quality}: {count} ({count/len(analysis_df)*100:.1f}%)\")\n",
        "\n",
        "        print(\"\\nAssertion Type Distribution:\")\n",
        "        type_counts = assertion_details_df['type'].value_counts()\n",
        "        for type_, count in type_counts.items():\n",
        "            print(f\"  {type_}: {count} ({count/len(assertion_details_df)*100:.1f}%)\")\n",
        "\n",
        "        return assertion_details_df\n",
        "    else:\n",
        "        print(\"No assertion details could be extracted for the refinement dataset.\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "nBaD67UPq75b"
      },
      "id": "nBaD67UPq75b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assertion_dataset = analyze_assertions_for_refinement(BASE_DRIVE_PATH)"
      ],
      "metadata": {
        "id": "8qKUPc_kr3ei"
      },
      "id": "8qKUPc_kr3ei",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of Assertion Refinement Techniques\n",
        "\n",
        "def improve_assertion_with_pattern_based_refinement(assertion_text, assertion_type):\n",
        "    \"\"\"\n",
        "    Implements pattern-based refinement to improve weak assertions.\n",
        "    This is a rule-based approach that replaces weak patterns with stronger alternatives.\n",
        "    \"\"\"\n",
        "    improved_assertion = assertion_text\n",
        "\n",
        "    # Pattern 1: Replace simple existence checks with more specific assertions\n",
        "    if assertion_type == 'existence' and 'is not None' in assertion_text:\n",
        "        # Extract the expression being checked\n",
        "        expr = assertion_text.split('assert ')[1].split(' is not None')[0].strip()\n",
        "        if expr:\n",
        "            improved_assertion = f\"assert isinstance({expr}, (list, dict, str)), 'Expected {expr} to be a container type'\"\n",
        "\n",
        "    # Pattern 2: Enhance equality checks with type checks\n",
        "    elif assertion_type == 'equality' and '==' in assertion_text and 'isinstance' not in assertion_text:\n",
        "        parts = assertion_text.split('==')\n",
        "        if len(parts) == 2:\n",
        "            left = parts[0].replace('assert', '').strip()\n",
        "            right = parts[1].strip()\n",
        "            if left and right and not right.isdigit():  # Don't add type check for numeric constants\n",
        "                improved_assertion = f\"assert isinstance({left}, type({right})), 'Type mismatch'\\nassert {left} == {right}, 'Value mismatch'\"\n",
        "\n",
        "    # Pattern 3: Add boundary checks to numeric assertions\n",
        "    elif assertion_type == 'comparison' and ('>' in assertion_text or '<' in assertion_text):\n",
        "        # Example: assert x > 0 -> assert x > 0, f\"Expected {x} to be positive\"\n",
        "        parts = re.split(r'[<>]=?', assertion_text)\n",
        "        if len(parts) == 2:\n",
        "            expr = parts[0].replace('assert', '').strip()\n",
        "            boundary = parts[1].strip()\n",
        "            if expr and boundary:\n",
        "                # Extract the comparison operator\n",
        "                operator = re.search(r'([<>]=?)', assertion_text).group(1)\n",
        "                # Add a descriptive message\n",
        "                if operator == '>':\n",
        "                    message = f\"Expected {expr} to be greater than {boundary}\"\n",
        "                elif operator == '>=':\n",
        "                    message = f\"Expected {expr} to be at least {boundary}\"\n",
        "                elif operator == '<':\n",
        "                    message = f\"Expected {expr} to be less than {boundary}\"\n",
        "                elif operator == '<=':\n",
        "                    message = f\"Expected {expr} to be at most {boundary}\"\n",
        "\n",
        "                improved_assertion = f\"assert {expr} {operator} {boundary}, '{message}'\"\n",
        "\n",
        "    # Pattern 4: Enhance collection checks\n",
        "    elif assertion_type == 'length_check' and 'len(' in assertion_text:\n",
        "        expr = re.search(r'len\\((.*?)\\)', assertion_text).group(1)\n",
        "        if expr:\n",
        "            # Add type check before length check\n",
        "            if '==' in assertion_text:\n",
        "                length = assertion_text.split('==')[1].strip()\n",
        "                improved_assertion = f\"assert isinstance({expr}, (list, tuple, dict, str)), '{expr} should be a collection'\\nassert len({expr}) == {length}, '{expr} should have length {length}'\"\n",
        "            elif '>' in assertion_text:\n",
        "                length = assertion_text.split('>')[1].strip()\n",
        "                improved_assertion = f\"assert isinstance({expr}, (list, tuple, dict, str)), '{expr} should be a collection'\\nassert len({expr}) > {length}, '{expr} should have length greater than {length}'\"\n",
        "\n",
        "    # Pattern 5: Add descriptive messages to assertions that don't have them\n",
        "    if ',' not in improved_assertion and not improved_assertion.endswith(')'):\n",
        "        improved_assertion += f\", 'Assertion failed: {improved_assertion.replace('assert ', '')}'\"\n",
        "\n",
        "    return improved_assertion\n",
        "\n",
        "def improve_assertion_with_semantic_refinement(assertion_text, function_code, context=None):\n",
        "    \"\"\"\n",
        "    Implements semantic refinement to generate more meaningful assertions.\n",
        "    This analyzes function purpose to create assertions that better capture intended behavior.\n",
        "    \"\"\"\n",
        "    # Parse the function to understand its purpose\n",
        "    try:\n",
        "        # Parse both function code and existing assertion\n",
        "        function_ast = ast.parse(function_code)\n",
        "        assertion_ast = ast.parse(assertion_text)\n",
        "\n",
        "        function_name = None\n",
        "        return_type = None\n",
        "        function_args = []\n",
        "\n",
        "        # Extract function details\n",
        "        for node in ast.walk(function_ast):\n",
        "            if isinstance(node, ast.FunctionDef):\n",
        "                function_name = node.name\n",
        "                function_args = [arg.arg for arg in node.args.args]\n",
        "\n",
        "                # Check for return type hints if available\n",
        "                if node.returns:\n",
        "                    return_type = ast.unparse(node.returns)\n",
        "\n",
        "                # Check docstring for return type info\n",
        "                if node.body and isinstance(node.body[0], ast.Expr) and isinstance(node.body[0].value, ast.Constant):\n",
        "                    docstring = node.body[0].value.value\n",
        "                    if \"return\" in docstring.lower():\n",
        "                        # Simple regex to find return type in docstring\n",
        "                        match = re.search(r\"return[s]?:?\\s*([\\w\\[\\]]+)\", docstring.lower())\n",
        "                        if match:\n",
        "                            return_type = match.group(1)\n",
        "\n",
        "                # Analyze return statements to infer return type\n",
        "                for subnode in ast.walk(node):\n",
        "                    if isinstance(subnode, ast.Return) and subnode.value:\n",
        "                        if isinstance(subnode.value, ast.List):\n",
        "                            return_type = \"list\"\n",
        "                        elif isinstance(subnode.value, ast.Dict):\n",
        "                            return_type = \"dict\"\n",
        "                        elif isinstance(subnode.value, ast.Str):\n",
        "                            return_type = \"str\"\n",
        "                        elif isinstance(subnode.value, ast.Num):\n",
        "                            return_type = \"number\"\n",
        "                        elif isinstance(subnode.value, ast.NameConstant) and subnode.value.value in [True, False]:\n",
        "                            return_type = \"bool\"\n",
        "                break\n",
        "\n",
        "        # Semantic improvements based on function analysis\n",
        "        improved_assertion = assertion_text\n",
        "\n",
        "        # If we have type information, use it to improve the assertion\n",
        "        if return_type:\n",
        "            if return_type == \"list\" and \"isinstance\" not in improved_assertion:\n",
        "                # Add appropriate checks for list return type\n",
        "                if any(arg in assertion_text for arg in function_args):\n",
        "                    func_call = re.search(r'assert\\s+([\\w\\._]+\\(.*?\\))', assertion_text)\n",
        "                    if func_call:\n",
        "                        func_expr = func_call.group(1)\n",
        "                        improved_assertion = f\"assert isinstance({func_expr}, list), 'Expected a list return value'\\n\"\n",
        "                        improved_assertion += f\"assert len({func_expr}) > 0, 'Expected non-empty list'\\n\"\n",
        "                        improved_assertion += assertion_text\n",
        "\n",
        "            elif return_type == \"dict\" and \"isinstance\" not in improved_assertion:\n",
        "                # Add appropriate checks for dictionary return type\n",
        "                if any(arg in assertion_text for arg in function_args):\n",
        "                    func_call = re.search(r'assert\\s+([\\w\\._]+\\(.*?\\))', assertion_text)\n",
        "                    if func_call:\n",
        "                        func_expr = func_call.group(1)\n",
        "                        improved_assertion = f\"assert isinstance({func_expr}, dict), 'Expected a dict return value'\\n\"\n",
        "                        improved_assertion += assertion_text\n",
        "\n",
        "            elif return_type == \"bool\" and \"==\" not in improved_assertion:\n",
        "                # Make boolean assertions more explicit\n",
        "                if \"is True\" in assertion_text or \"is not False\" in assertion_text:\n",
        "                    func_call = re.search(r'assert\\s+([\\w\\._]+\\(.*?\\))', assertion_text)\n",
        "                    if func_call:\n",
        "                        func_expr = func_call.group(1)\n",
        "                        improved_assertion = f\"assert {func_expr} == True, 'Expected function to return True'\"\n",
        "\n",
        "        # If assertion is checking exception but doesn't use pytest.raises\n",
        "        if \"try:\" in assertion_text and \"except\" in assertion_text and \"pytest\" not in assertion_text:\n",
        "            improved_assertion = \"import pytest\\n\"\n",
        "            exception_type = re.search(r'except\\s+(\\w+):', assertion_text)\n",
        "            if exception_type:\n",
        "                exception = exception_type.group(1)\n",
        "                func_call = re.search(r'(\\w+\\(.*?\\))', assertion_text)\n",
        "                if func_call:\n",
        "                    func_expr = func_call.group(1)\n",
        "                    improved_assertion += f\"with pytest.raises({exception}):\\n    {func_expr}\"\n",
        "\n",
        "        return improved_assertion\n",
        "\n",
        "    except SyntaxError:\n",
        "        # If we can't parse the code, just return the original assertion\n",
        "        return assertion_text\n",
        "\n",
        "def apply_hybrid_assertion_refinement(assertion_text, function_code, original_quality='weak'):\n",
        "    \"\"\"\n",
        "    Implements the hybrid approach combining pattern-based and semantic refinement.\n",
        "    This two-stage process first applies pattern-based improvements then semantic enhancements.\n",
        "    \"\"\"\n",
        "    # Identify the assertion type\n",
        "    assertion_type = 'general'\n",
        "    if 'assert isinstance' in assertion_text:\n",
        "        assertion_type = 'type_check'\n",
        "    elif 'assert len' in assertion_text:\n",
        "        assertion_type = 'length_check'\n",
        "    elif '==' in assertion_text:\n",
        "        assertion_type = 'equality'\n",
        "    elif 'is not None' in assertion_text:\n",
        "        assertion_type = 'existence'\n",
        "    elif 'is None' in assertion_text:\n",
        "        assertion_type = 'absence'\n",
        "    elif 'in ' in assertion_text:\n",
        "        assertion_type = 'membership'\n",
        "    elif 'not in ' in assertion_text:\n",
        "        assertion_type = 'exclusion'\n",
        "    elif '>' in assertion_text or '<' in assertion_text:\n",
        "        assertion_type = 'comparison'\n",
        "\n",
        "    # Stage 1: Apply pattern-based refinement\n",
        "    pattern_improved = improve_assertion_with_pattern_based_refinement(assertion_text, assertion_type)\n",
        "\n",
        "    # Stage 2: Apply semantic refinement if needed\n",
        "    if pattern_improved == assertion_text or original_quality == 'weak':\n",
        "        final_improved = improve_assertion_with_semantic_refinement(pattern_improved, function_code)\n",
        "        return final_improved\n",
        "    else:\n",
        "        return pattern_improved\n",
        "\n",
        "# Example usage function\n",
        "def demonstrate_assertion_refinement(BASE_DRIVE_PATH):\n",
        "    \"\"\"Demonstrate the assertion refinement process with examples from the dataset.\"\"\"\n",
        "    print(\"\\n--- Demonstrating Hybrid Assertion Refinement Approach ---\")\n",
        "\n",
        "    # Load the dataset if available\n",
        "    dataset_path = os.path.join(BASE_DRIVE_PATH, \"assertion_refinement_dataset\", \"full_dataset.csv\")\n",
        "    if os.path.exists(dataset_path):\n",
        "        df = pd.read_csv(dataset_path)\n",
        "        print(f\"Loaded {len(df)} assertions from refinement dataset.\")\n",
        "\n",
        "        # Select some weak assertions for demonstration\n",
        "        weak_assertions = df[df['quality'] == 'weak'].sample(min(5, len(df[df['quality'] == 'weak'])))\n",
        "\n",
        "        if not weak_assertions.empty:\n",
        "            print(\"\\nExample Assertion Refinements:\")\n",
        "            for i, row in enumerate(weak_assertions.iterrows(), 1):\n",
        "                _, assertion = row\n",
        "\n",
        "                # Load the original function code\n",
        "                original_code = \"def example_func():\\n    return True\"  # Default placeholder\n",
        "\n",
        "                # In a real implementation, you would load the actual function code from your test files\n",
        "                # using the unique_id to locate it\n",
        "\n",
        "                print(f\"\\nExample {i}:\")\n",
        "                print(f\"Original assertion ({assertion['type']}): {assertion['text']}\")\n",
        "\n",
        "                # Apply the hybrid refinement approach\n",
        "                improved = apply_hybrid_assertion_refinement(\n",
        "                    assertion['text'],\n",
        "                    original_code,\n",
        "                    assertion['quality']\n",
        "                )\n",
        "\n",
        "                print(f\"Improved assertion: {improved}\")\n",
        "        else:\n",
        "            print(\"No weak assertions found in the dataset for demonstration.\")\n",
        "    else:\n",
        "        print(f\"Assertion refinement dataset not found at {dataset_path}.\")\n",
        "        print(\"Run the assertion analysis function first to create the dataset.\")\n",
        "\n",
        "        # Demonstrate with some example assertions\n",
        "        print(\"\\nDemonstrating with example assertions:\")\n",
        "\n",
        "        examples = [\n",
        "            (\"assert result is not None\", \"def get_user_data(user_id):\\n    return {'id': user_id, 'name': 'Test'}\", \"existence\"),\n",
        "            (\"assert len(items) > 0\", \"def fetch_items(category):\\n    return ['item1', 'item2']\", \"length_check\"),\n",
        "            (\"assert calculate_total(10, 20) == 30\", \"def calculate_total(a, b):\\n    return a + b\", \"equality\")\n",
        "        ]\n",
        "\n",
        "        for i, (assertion, function, type_) in enumerate(examples, 1):\n",
        "            print(f\"\\nExample {i}:\")\n",
        "            print(f\"Original assertion ({type_}): {assertion}\")\n",
        "            improved = apply_hybrid_assertion_refinement(assertion, function, 'weak')\n",
        "            print(f\"Improved assertion: {improved}\")\n"
      ],
      "metadata": {
        "id": "oEiUpLaireSi"
      },
      "id": "oEiUpLaireSi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demonstrate_assertion_refinement(BASE_DRIVE_PATH)"
      ],
      "metadata": {
        "id": "mF5T26YCrxvZ"
      },
      "id": "mF5T26YCrxvZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of Advanced Assertion Correction with LLM\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import ast\n",
        "import time\n",
        "import re\n",
        "import random\n",
        "import sys\n",
        "import subprocess\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "id": "LSYcZHB1sBXh"
      },
      "id": "LSYcZHB1sBXh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Helper: Extract Assertions with Context ---\n",
        "def extract_assertions_with_context(code_string):\n",
        "    \"\"\"Extract assertions from code string with their line numbers, context, and type classification.\"\"\"\n",
        "    assertions = []\n",
        "    if not isinstance(code_string, str):\n",
        "        return assertions\n",
        "\n",
        "    try:\n",
        "        tree = ast.parse(code_string)\n",
        "        for node in ast.walk(tree):\n",
        "            if isinstance(node, ast.Assert):\n",
        "                try:\n",
        "                    assertion_text = ast.unparse(node.test).strip()\n",
        "                    line_number = node.lineno\n",
        "\n",
        "                    # Classify assertion type\n",
        "                    assertion_type = \"unknown\"\n",
        "\n",
        "                    # Check for equality assertions\n",
        "                    if isinstance(node.test, ast.Compare) and any(isinstance(op, ast.Eq) for op in node.test.ops):\n",
        "                        assertion_type = \"equality\"\n",
        "                    # Check for type assertions\n",
        "                    elif (isinstance(node.test, ast.Call) and\n",
        "                          isinstance(node.test.func, ast.Name) and\n",
        "                          node.test.func.id == \"isinstance\"):\n",
        "                        assertion_type = \"type_check\"\n",
        "                    # Check for membership assertions\n",
        "                    elif isinstance(node.test, ast.Compare) and any(isinstance(op, ast.In) for op in node.test.ops):\n",
        "                        assertion_type = \"membership\"\n",
        "                    # Check for length/emptiness assertions\n",
        "                    elif (isinstance(node.test, ast.Compare) and\n",
        "                          isinstance(node.test.left, ast.Call) and\n",
        "                          isinstance(node.test.left.func, ast.Name) and\n",
        "                          node.test.left.func.id == \"len\"):\n",
        "                        assertion_type = \"length_check\"\n",
        "\n",
        "                    # Get surrounding context (the test function containing this assertion)\n",
        "                    containing_function = None\n",
        "                    for potential_parent in ast.walk(tree):\n",
        "                        if (isinstance(potential_parent, ast.FunctionDef) and\n",
        "                            potential_parent.lineno <= node.lineno and\n",
        "                            hasattr(potential_parent, 'end_lineno') and\n",
        "                            potential_parent.end_lineno >= node.lineno):\n",
        "                            containing_function = potential_parent.name\n",
        "                            break\n",
        "\n",
        "                    assertions.append({\n",
        "                        \"text\": assertion_text,\n",
        "                        \"line\": line_number,\n",
        "                        \"type\": assertion_type,\n",
        "                        \"containing_function\": containing_function\n",
        "                    })\n",
        "                except AttributeError:\n",
        "                    assertions.append({\n",
        "                        \"text\": f\"Assertion involving node type: {type(node.test).__name__}\",\n",
        "                        \"line\": node.lineno,\n",
        "                        \"type\": \"unknown\",\n",
        "                        \"containing_function\": None\n",
        "                    })\n",
        "    except SyntaxError:\n",
        "        pass\n",
        "\n",
        "    return assertions\n",
        "\n",
        "def analyze_function_for_testability(function_code):\n",
        "    \"\"\"Analyzes the function to determine what aspects should be tested.\"\"\"\n",
        "    analysis = {\n",
        "        \"return_type\": None,\n",
        "        \"parameters\": [],\n",
        "        \"edge_cases\": [],\n",
        "        \"complex_logic\": False\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        tree = ast.parse(function_code)\n",
        "        for node in ast.walk(tree):\n",
        "            # Extract function parameters\n",
        "            if isinstance(node, ast.FunctionDef):\n",
        "                analysis[\"parameters\"] = [arg.arg for arg in node.args.args]\n",
        "\n",
        "                # Check for docstring that might provide hints\n",
        "                if (node.body and isinstance(node.body[0], ast.Expr) and\n",
        "                    isinstance(node.body[0].value, ast.Constant) and\n",
        "                    isinstance(node.body[0].value.value, str)):\n",
        "                    docstring = node.body[0].value.value\n",
        "                    if \"return\" in docstring.lower():\n",
        "                        # Try to extract return type from docstring\n",
        "                        return_match = re.search(r\"return[s]?:?\\s+([\\w\\[\\]]+)\", docstring.lower())\n",
        "                        if return_match:\n",
        "                            analysis[\"return_type\"] = return_match.group(1)\n",
        "\n",
        "                # Detect complex logic - conditionals, loops, multiple returns\n",
        "                has_conditionals = any(isinstance(n, (ast.If, ast.IfExp)) for n in ast.walk(node))\n",
        "                has_loops = any(isinstance(n, (ast.For, ast.While)) for n in ast.walk(node))\n",
        "                returns = [n for n in ast.walk(node) if isinstance(n, ast.Return)]\n",
        "\n",
        "                analysis[\"complex_logic\"] = has_conditionals or has_loops or len(returns) > 1\n",
        "\n",
        "                # Find returns to determine return type if not already found\n",
        "                for ret_node in returns:\n",
        "                    if ret_node.value:\n",
        "                        if isinstance(ret_node.value, ast.List):\n",
        "                            analysis[\"return_type\"] = \"list\"\n",
        "                        elif isinstance(ret_node.value, ast.Dict):\n",
        "                            analysis[\"return_type\"] = \"dict\"\n",
        "                        elif isinstance(ret_node.value, ast.Constant) and isinstance(ret_node.value.value, str):\n",
        "                            analysis[\"return_type\"] = \"str\"\n",
        "                        elif isinstance(ret_node.value, ast.Constant) and isinstance(ret_node.value.value, (int, float)):\n",
        "                            analysis[\"return_type\"] = \"numeric\"\n",
        "                        elif isinstance(ret_node.value, ast.Constant) and isinstance(ret_node.value.value, bool):\n",
        "                            analysis[\"return_type\"] = \"bool\"\n",
        "                        # Add more type detections as needed\n",
        "\n",
        "            # Identify potential edge cases from conditionals\n",
        "            if isinstance(node, ast.If):\n",
        "                if (isinstance(node.test, ast.Compare) and\n",
        "                    any(isinstance(op, (ast.Eq, ast.NotEq, ast.Lt, ast.LtE, ast.Gt, ast.GtE))\n",
        "                        for op in node.test.ops)):\n",
        "                    if hasattr(node.test, 'comparators') and node.test.comparators:\n",
        "                        comp = node.test.comparators[0]\n",
        "                        if isinstance(comp, ast.Constant) and comp.value in (0, 1, \"\", [], {}, None):\n",
        "                            analysis[\"edge_cases\"].append(f\"Value equals {comp.value}\")\n",
        "\n",
        "            # Check for error handling\n",
        "            if isinstance(node, ast.Try):\n",
        "                analysis[\"edge_cases\"].append(\"Error handling\")\n",
        "\n",
        "    except SyntaxError:\n",
        "        pass\n",
        "\n",
        "    return analysis\n",
        "\n",
        "def analyze_assertion_quality(assertions, function_code):\n",
        "    \"\"\"Analyze the quality of assertions relative to the function being tested.\"\"\"\n",
        "    analysis = {\n",
        "        \"total_assertions\": len(assertions),\n",
        "        \"assertions_with_messages\": sum(1 for a in assertions if a.get(\"has_message\", False)),\n",
        "        \"weak_assertions\": [],\n",
        "        \"highlights\": []\n",
        "    }\n",
        "\n",
        "    # Identify weak assertion patterns\n",
        "    weak_patterns = [\n",
        "        r\"is not None\",\n",
        "        r\"is None\",\n",
        "        r\"== True\",\n",
        "        r\"== False\",\n",
        "        r\"isinstance\\(.*\\)\",\n",
        "        r\"in \",\n",
        "        r\"len\\(.*\\) [><=]\"\n",
        "    ]\n",
        "\n",
        "    for assertion in assertions:\n",
        "        assertion_text = assertion.get(\"assertion\", \"\")\n",
        "        for pattern in weak_patterns:\n",
        "            if re.search(pattern, assertion_text):\n",
        "                analysis[\"weak_assertions\"].append(assertion)\n",
        "                break\n",
        "\n",
        "    # Generate highlights based on analysis\n",
        "    if analysis[\"total_assertions\"] == 0:\n",
        "        analysis[\"highlights\"].append(\"No assertions found in test code\")\n",
        "    else:\n",
        "        if len(analysis[\"weak_assertions\"]) > 0:\n",
        "            analysis[\"highlights\"].append(f\"Found {len(analysis['weak_assertions'])} weak assertions that should be strengthened\")\n",
        "\n",
        "        if analysis[\"assertions_with_messages\"] == 0:\n",
        "            analysis[\"highlights\"].append(\"No assertions have error messages\")\n",
        "\n",
        "        # Try to detect if assertions might be missing important function aspects\n",
        "        try:\n",
        "            func_ast = ast.parse(function_code)\n",
        "            has_conditionals = any(isinstance(node, ast.If) for node in ast.walk(func_ast))\n",
        "            has_loops = any(isinstance(node, (ast.For, ast.While)) for node in ast.walk(func_ast))\n",
        "\n",
        "            if has_conditionals and analysis[\"total_assertions\"] < 2:\n",
        "                analysis[\"highlights\"].append(\"Function contains conditional logic but may be undertested\")\n",
        "            if has_loops and analysis[\"total_assertions\"] < 2:\n",
        "                analysis[\"highlights\"].append(\"Function contains loops but may be undertested\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return analysis\n",
        "\n",
        "def analyze_survived_mutations(function_code, test_code):\n",
        "    \"\"\"Analyze which types of mutations likely survived based on function and test code.\"\"\"\n",
        "    # This is a simplified placeholder. In a real implementation,\n",
        "    # you would examine the actual mutation results in detail.\n",
        "    survived = {\n",
        "        \"comparison_operators\": 0,\n",
        "        \"arithmetic_operators\": 0,\n",
        "        \"constant_values\": 0,\n",
        "        \"logical_operators\": 0\n",
        "    }\n",
        "\n",
        "    # Simple heuristic analysis - can be much more sophisticated\n",
        "    try:\n",
        "        func_ast = ast.parse(function_code)\n",
        "\n",
        "        # Count potential mutation targets\n",
        "        comparisons = sum(1 for _ in ast.walk(func_ast) if isinstance(_, ast.Compare))\n",
        "        arithmetic = sum(1 for _ in ast.walk(func_ast) if isinstance(_, ast.BinOp) and\n",
        "                        isinstance(_.op, (ast.Add, ast.Sub, ast.Mult, ast.Div)))\n",
        "        constants = sum(1 for _ in ast.walk(func_ast) if isinstance(_, ast.Constant) and\n",
        "                      isinstance(_.value, (int, float)))\n",
        "        logical = sum(1 for _ in ast.walk(func_ast) if isinstance(_, ast.BoolOp))\n",
        "\n",
        "        # Estimate survived mutations based on what's in the function vs what's tested\n",
        "        test_ast = ast.parse(test_code)\n",
        "        test_eq_count = sum(1 for _ in ast.walk(test_ast) if isinstance(_, ast.Compare) and\n",
        "                          any(isinstance(op, ast.Eq) for op in getattr(_, 'ops', [])))\n",
        "\n",
        "        survived[\"comparison_operators\"] = max(0, comparisons - test_eq_count)\n",
        "        survived[\"arithmetic_operators\"] = arithmetic // 2  # Estimate\n",
        "        survived[\"constant_values\"] = constants // 2  # Estimate\n",
        "        survived[\"logical_operators\"] = logical\n",
        "    except:\n",
        "        # Default values if analysis fails\n",
        "        survived = {\n",
        "            \"comparison_operators\": 1,\n",
        "            \"arithmetic_operators\": 1,\n",
        "            \"constant_values\": 1,\n",
        "            \"logical_operators\": 1\n",
        "        }\n",
        "\n",
        "    return survived"
      ],
      "metadata": {
        "id": "h-7t-vx61HTd"
      },
      "id": "h-7t-vx61HTd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "wait"
      ],
      "metadata": {
        "id": "vi9gtmJG8Iow"
      },
      "id": "vi9gtmJG8Iow"
    },
    {
      "cell_type": "code",
      "source": [
        "# Assertion Guidance and Context Formatting Functions\n",
        "\n",
        "def generate_assertion_guidance(function_analysis, original_assertions, mutation_results):\n",
        "    \"\"\"Generates guidance for assertion improvement based on function analysis and mutation results.\"\"\"\n",
        "    guidance = [\"Based on the function analysis, consider the following improvements:\"]\n",
        "\n",
        "    # Check if we have edge cases covered\n",
        "    if function_analysis.get(\"edge_cases\"):\n",
        "        guidance.append(\"- Add assertions for these edge cases:\")\n",
        "        for case in function_analysis.get(\"edge_cases\"):\n",
        "            guidance.append(f\"  * {case}\")\n",
        "\n",
        "    # Provide type-specific guidance\n",
        "    return_type = function_analysis.get(\"return_type\")\n",
        "    if return_type:\n",
        "        if return_type == \"list\":\n",
        "            guidance.append(\"- For list returns, add assertions checking:\")\n",
        "            guidance.append(\"  * List length\")\n",
        "            guidance.append(\"  * Expected elements\")\n",
        "            guidance.append(\"  * Element types\")\n",
        "            guidance.append(\"  * Element ordering (if relevant)\")\n",
        "        elif return_type == \"dict\":\n",
        "            guidance.append(\"- For dictionary returns, add assertions checking:\")\n",
        "            guidance.append(\"  * Dictionary keys presence\")\n",
        "            guidance.append(\"  * Value types for each key\")\n",
        "            guidance.append(\"  * Complete structure verification\")\n",
        "        elif return_type == \"str\":\n",
        "            guidance.append(\"- For string returns, add assertions checking:\")\n",
        "            guidance.append(\"  * Expected string value\")\n",
        "            guidance.append(\"  * String length\")\n",
        "            guidance.append(\"  * String content patterns (using regular expressions)\")\n",
        "        elif return_type == \"bool\":\n",
        "            guidance.append(\"- For boolean returns, add assertions checking:\")\n",
        "            guidance.append(\"  * True/False for various input combinations\")\n",
        "            guidance.append(\"  * Boundary conditions that might affect the result\")\n",
        "        elif return_type == \"numeric\":\n",
        "            guidance.append(\"- For numeric returns, add assertions checking:\")\n",
        "            guidance.append(\"  * Exact expected values\")\n",
        "            guidance.append(\"  * Range constraints (min/max)\")\n",
        "            guidance.append(\"  * Type verification (int vs float)\")\n",
        "\n",
        "    # Add guidance for complex logic\n",
        "    if function_analysis.get(\"complex_logic\"):\n",
        "        guidance.append(\"- Function has complex logic, add assertions for:\")\n",
        "        guidance.append(\"  * Each conditional branch\")\n",
        "        guidance.append(\"  * Loop boundary cases (empty, single item, multiple items)\")\n",
        "        guidance.append(\"  * All possible return paths\")\n",
        "\n",
        "    # Add guidance based on mutation results\n",
        "    if mutation_results and mutation_results.get(\"survived_mutations\"):\n",
        "        guidance.append(\"- Based on mutation testing, strengthen assertions for:\")\n",
        "        survived_mutations = mutation_results.get(\"survived_mutations\", {})\n",
        "\n",
        "        # Parse the survived mutations which may be a string representation of a dict\n",
        "        if isinstance(survived_mutations, str):\n",
        "            try:\n",
        "                survived_mutations = eval(survived_mutations)\n",
        "            except:\n",
        "                survived_mutations = {}\n",
        "\n",
        "        for mutation_type, count in survived_mutations.items():\n",
        "            if \"comparison\" in mutation_type.lower():\n",
        "                guidance.append(\"  * More precise comparisons (check exact values, not just existence)\")\n",
        "            if \"constant\" in mutation_type.lower() or \"boundary\" in mutation_type.lower():\n",
        "                guidance.append(\"  * Boundary values and exact constant values\")\n",
        "            if \"arithmetic\" in mutation_type.lower():\n",
        "                guidance.append(\"  * Calculated values with precise expected results\")\n",
        "            if \"logical\" in mutation_type.lower():\n",
        "                guidance.append(\"  * Logical conditions (and/or operators)\")\n",
        "\n",
        "    # Check for any missing assertion types\n",
        "    assertion_types = {a.get(\"type\") for a in original_assertions}\n",
        "    if \"type_check\" not in assertion_types:\n",
        "        guidance.append(\"- Add type verification assertions using isinstance()\")\n",
        "    if \"length_check\" not in assertion_types and return_type in [\"list\", \"dict\", \"str\"]:\n",
        "        guidance.append(\"- Add assertions checking the length of returned collections\")\n",
        "\n",
        "    return \"\\n\".join(guidance)\n",
        "\n",
        "def format_assertion_context(assertions):\n",
        "    \"\"\"Formats the assertion context for the prompt.\"\"\"\n",
        "    if not assertions:\n",
        "        return \"No assertions found in the original test code.\"\n",
        "\n",
        "    context = f\"Found {len(assertions)} assertions in the original test code:\\n\"\n",
        "    for i, assertion in enumerate(assertions):\n",
        "        context += f\"  {i+1}. Line {assertion.get('line')} ({assertion.get('type')} assertion): assert {assertion.get('text')}\"\n",
        "        if assertion.get(\"containing_function\"):\n",
        "            context += f\" in function {assertion.get('containing_function')}\"\n",
        "        context += \"\\n\"\n",
        "\n",
        "    return context"
      ],
      "metadata": {
        "id": "gF5WW7CfXcME"
      },
      "id": "gF5WW7CfXcME",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced Prompt Template for LLM\n",
        "\n",
        "enhanced_prompt_template = \"\"\"You are an expert Python test engineer specializing in creating robust, high-quality unit tests.\n",
        "\n",
        "FUNCTION TO TEST:\n",
        "```python\n",
        "{function_code}\n",
        "```\n",
        "\n",
        "ORIGINAL TEST CODE WITH ISSUES:\n",
        "```python\n",
        "{original_test_code}\n",
        "```\n",
        "\n",
        "ASSERTION ANALYSIS:\n",
        "{assertion_context}\n",
        "\n",
        "MUTATION TESTING RESULTS:\n",
        "{mutation_analysis}\n",
        "\n",
        "GUIDANCE FOR IMPROVEMENT:\n",
        "{assertion_guidance}\n",
        "\n",
        "Your task is to rewrite the test code to maximize assertion accuracy and test coverage. Follow these specific guidelines:\n",
        "\n",
        "IMPLEMENT PATTERN-BASED REFINEMENT:\n",
        "- Replace weak assertions like `assert some_function() is not None` with stronger assertions.\n",
        "- Add boundary checks for numeric assertions (e.g., test min/max values, zero, negative values).\n",
        "- Combine type checks with value checks (e.g., `assert isinstance(result, list) and len(result) > 0`).\n",
        "- Convert simple checks to more comprehensive validations.\n",
        "\n",
        "IMPLEMENT SEMANTIC REFINEMENT:\n",
        "- Add assertions that verify the function's core purpose.\n",
        "- Create assertions for each logical path through the function.\n",
        "- Ensure all edge cases identified in the guidance are tested.\n",
        "- Include assertions that verify both positive and negative test cases.\n",
        "\n",
        "BEST PRACTICES FOR ASSERTIONS:\n",
        "- Add clear error messages to assertions: `assert condition, \"Helpful error message\"`.\n",
        "- Use specific, focused assertions rather than complex compound assertions.\n",
        "- Test both the structure and content of returned collections.\n",
        "- For collections, check types, lengths, and specific elements.\n",
        "- For functions with side effects, verify the side effect occurred properly.\n",
        "\n",
        "MUTATION TESTING COVERAGE:\n",
        "- Add assertions specifically designed to detect the survived mutations mentioned.\n",
        "- Use precise equality checks rather than existence checks.\n",
        "- Verify exact values, not just approximate results.\n",
        "- Include assertions that would detect off-by-one errors.\n",
        "\n",
        "Ensure the corrected test code:\n",
        "- Is syntactically correct and runnable.\n",
        "- Maintains the same test function structure (don't rename test functions).\n",
        "- Includes significantly stronger assertions.\n",
        "- Has clear, readable code with appropriate comments.\n",
        "\n",
        "RETURN ONLY THE IMPROVED TEST CODE, with no explanations, wrapped in python and ``` markers.\n",
        "\n",
        "# Your improved test code here\n",
        "```python\n",
        "# [Your improved test code will be generated here by the LLM]\n",
        "```\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "8igOa6eEXfMh"
      },
      "id": "8igOa6eEXfMh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced Assertion Correction and LLM-based Test Correction Functions\n",
        "\n",
        "# Ensure that extract_assertions_with_context, analyze_function_for_testability,\n",
        "# generate_assertion_guidance, format_assertion_context, and enhanced_prompt_template\n",
        "# from previous cells are defined and available in the Colab environment before running this cell.\n",
        "\n",
        "def advanced_assertion_correction(llm_pipeline, function_code, original_test_code,\n",
        "                               mutation_results, enhanced_prompt_template, max_new_tokens=1024):\n",
        "    \"\"\"\n",
        "    Advanced assertion correction technique using LLMs with specialized prompts.\n",
        "    Implements the hybrid approach combining pattern-based and semantic refinement.\n",
        "\n",
        "    Args:\n",
        "        llm_pipeline: The LLM pipeline to use for correction.\n",
        "        function_code: The function code being tested.\n",
        "        original_test_code: The original tests with weak assertions.\n",
        "        mutation_results: Information about mutation testing results.\n",
        "        enhanced_prompt_template: The specialized prompt template.\n",
        "        max_new_tokens: Maximum tokens for generation.\n",
        "\n",
        "    Returns:\n",
        "        Corrected test code with improved assertions, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    # Extract and analyze the original assertions\n",
        "    # Note: extract_assertions_with_context should be defined from the user's provided code\n",
        "    original_assertions = extract_assertions_with_context(original_test_code)\n",
        "    assertion_analysis = {\n",
        "        \"total\": len(original_assertions),\n",
        "        \"by_type\": {}\n",
        "    }\n",
        "\n",
        "    # Group assertions by type for analysis\n",
        "    for assertion in original_assertions:\n",
        "        assertion_type = assertion.get(\"type\", \"unknown\") # Use .get for safety\n",
        "        if assertion_type not in assertion_analysis[\"by_type\"]:\n",
        "            assertion_analysis[\"by_type\"][assertion_type] = []\n",
        "        assertion_analysis[\"by_type\"][assertion_type].append(assertion)\n",
        "\n",
        "    # Analyze function code to understand what should be tested\n",
        "    # Note: analyze_function_for_testability should be defined from the user's provided code\n",
        "    function_analysis = analyze_function_for_testability(function_code)\n",
        "\n",
        "    # Generate assertion guidance based on function analysis and current assertions\n",
        "    # Note: generate_assertion_guidance is in next_code_cell_1.py\n",
        "    assertion_guidance = generate_assertion_guidance(function_analysis, original_assertions, mutation_results)\n",
        "\n",
        "    # Format assertion context for the prompt\n",
        "    # Note: format_assertion_context is in next_code_cell_1.py\n",
        "    assertion_context = format_assertion_context(original_assertions)\n",
        "\n",
        "    # Create a mutation analysis summary if provided\n",
        "    mutation_analysis_summary = \"\"\n",
        "    if mutation_results:\n",
        "        killed_ratio = f\"{mutation_results.get('mutants_killed', 0)}/{mutation_results.get('total_mutants_generated', 0)}\"\n",
        "        mutation_analysis_summary = f\"Mutation testing results: {killed_ratio} mutants killed, score: {mutation_results.get('mutation_score', 0):.2f}%.\\n\"\n",
        "\n",
        "        if mutation_results.get('survived_mutations'):\n",
        "            mutation_analysis_summary += \"Types of mutations that survived original tests:\\n\"\n",
        "            for mutation_type, count in mutation_results.get('survived_mutations', {}).items():\n",
        "                mutation_analysis_summary += f\"- {mutation_type}: {count} instances\\n\"\n",
        "    else:\n",
        "        mutation_analysis_summary = \"No mutation analysis results provided.\"\n",
        "\n",
        "    # Complete the specialized prompt with all context information\n",
        "    # Note: enhanced_prompt_template is in next_code_cell_2.py\n",
        "    full_prompt = enhanced_prompt_template.format(\n",
        "        function_code=function_code,\n",
        "        original_test_code=original_test_code,\n",
        "        assertion_context=assertion_context,\n",
        "        mutation_analysis=mutation_analysis_summary, # Use the generated summary\n",
        "        assertion_guidance=assertion_guidance\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        model_identifier = getattr(getattr(llm_pipeline, \"model\", {}), \"config\", {}).get(\"_name_or_path\", \"Correction LLM\")\n",
        "        print(f\"Using advanced assertion correction with {model_identifier}...\")\n",
        "\n",
        "        # Generate the corrected test code\n",
        "        sequences = llm_pipeline(\n",
        "            full_prompt,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_return_sequences=1,\n",
        "            do_sample=True,\n",
        "            temperature=0.6,  # Balanced for creativity but precision\n",
        "            top_k=40,\n",
        "            top_p=0.95,\n",
        "            pad_token_id=llm_pipeline.tokenizer.eos_token_id\n",
        "        )\n",
        "        corrected_code_suggestion = sequences[0][\"generated_text\"]\n",
        "\n",
        "        # Extract the Python code block\n",
        "        code_start_marker = \"```python\\n\"\n",
        "        code_end_marker = \"\\n```\"\n",
        "        extracted_correction = \"\"\n",
        "\n",
        "        # Find the last occurrence of the code block markers\n",
        "        start_index = corrected_code_suggestion.rfind(code_start_marker)\n",
        "        if start_index != -1:\n",
        "            start_index += len(code_start_marker)\n",
        "            end_index = corrected_code_suggestion.find(code_end_marker, start_index)\n",
        "            if end_index != -1:\n",
        "                extracted_correction = corrected_code_suggestion[start_index:end_index]\n",
        "            else: # If end marker not found after the last start marker, take rest of string\n",
        "                extracted_correction = corrected_code_suggestion[start_index:]\n",
        "        else:\n",
        "            # Fallback: if no ```python block, assume the generation after prompt is the code\n",
        "            # This might happen if the LLM doesn't strictly follow the marker instruction\n",
        "            prompt_end_index = len(full_prompt)\n",
        "            if len(corrected_code_suggestion) > prompt_end_index:\n",
        "                extracted_correction = corrected_code_suggestion[prompt_end_index:].strip()\n",
        "            else:\n",
        "                # If the suggestion is not longer than the prompt, it likely failed or is empty\n",
        "                extracted_correction = corrected_code_suggestion.strip()\n",
        "\n",
        "        final_correction = extracted_correction.strip()\n",
        "\n",
        "        if not final_correction:\n",
        "            print(\"Warning: LLM did not return a code suggestion or the extraction failed.\")\n",
        "            return original_test_code # Return original if correction fails\n",
        "\n",
        "        # Perform a quick analysis of the corrections made\n",
        "        # Note: extract_assertions_with_context should be defined from the user's provided code\n",
        "        corrected_assertions = extract_assertions_with_context(final_correction)\n",
        "        print(f\"Original test had {len(original_assertions)} assertions, corrected test has {len(corrected_assertions)} assertions.\")\n",
        "\n",
        "        return final_correction\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during advanced assertion correction: {e}\")\n",
        "        return original_test_code # Return original in case of error\n",
        "\n",
        "def correct_test_code_with_llm(correction_llm_pipeline, function_code, original_test_code, context_data, prompt_template):\n",
        "    \"\"\"Use LLM to correct the test code based on enhanced prompt with detailed context.\"\"\"\n",
        "\n",
        "    # Format the prompt with all the context data\n",
        "    # Note: prompt_template here refers to enhanced_prompt_template from next_code_cell_2.py\n",
        "    formatted_prompt = prompt_template.format(\n",
        "        function_code=function_code,\n",
        "        original_test_code=original_test_code,\n",
        "        assertion_context=context_data.get(\"assertion_context\", \"No specific issues identified.\"),\n",
        "        mutation_analysis=context_data.get(\"mutation_analysis\", \"No mutation analysis available.\"),\n",
        "        assertion_guidance=context_data.get(\"assertion_guidance\", \"Improve assertion quality and coverage.\")\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        model_name = getattr(getattr(correction_llm_pipeline, \"model\", {}), \"config\", {}).get(\"_name_or_path\", \"Correction LLM\")\n",
        "        print(f\"Requesting corrected test code from {model_name}...\")\n",
        "\n",
        "        # Generate the corrected test code\n",
        "        response = correction_llm_pipeline(\n",
        "            formatted_prompt,\n",
        "            max_new_tokens=1024,\n",
        "            do_sample=True,\n",
        "            temperature=0.5,\n",
        "            top_p=0.95,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=correction_llm_pipeline.tokenizer.eos_token_id # Added pad_token_id\n",
        "        )\n",
        "\n",
        "        generated_text = response[0][\"generated_text\"]\n",
        "\n",
        "        # Extract the Python code from the response\n",
        "        corrected_code = \"\"\n",
        "        # Using the same robust extraction logic as in advanced_assertion_correction\n",
        "        code_start_marker = \"```python\\n\"\n",
        "        code_end_marker = \"\\n```\"\n",
        "\n",
        "        start_index = generated_text.rfind(code_start_marker)\n",
        "        if start_index != -1:\n",
        "            start_index += len(code_start_marker)\n",
        "            end_index = generated_text.find(code_end_marker, start_index)\n",
        "            if end_index != -1:\n",
        "                corrected_code = generated_text[start_index:end_index]\n",
        "            else:\n",
        "                corrected_code = generated_text[start_index:]\n",
        "        else:\n",
        "            prompt_end_index = len(formatted_prompt)\n",
        "            if len(generated_text) > prompt_end_index:\n",
        "                corrected_code = generated_text[prompt_end_index:].strip()\n",
        "            else:\n",
        "                corrected_code = generated_text.strip()\n",
        "\n",
        "        final_corrected_code = corrected_code.strip()\n",
        "\n",
        "        if not final_corrected_code:\n",
        "            print(\"Warning: Could not extract corrected code from LLM response. Returning original.\")\n",
        "            return original_test_code # Return original if extraction fails\n",
        "\n",
        "        print(f\"Successfully generated corrected test code ({len(final_corrected_code.splitlines())} lines).\")\n",
        "        return final_corrected_code\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating corrected test code: {str(e)}\")\n",
        "        return original_test_code # Return original in case of error"
      ],
      "metadata": {
        "id": "TQCJDl0uXlxp"
      },
      "id": "TQCJDl0uXlxp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Mutation Operators ---\n",
        "def mutate_comparison_operator(node):\n",
        "    \"\"\"Mutates comparison operators (e.g., > to >=, == to !=).\"\"\"\n",
        "    if isinstance(node, ast.Compare):\n",
        "        original_op = type(node.ops[0])\n",
        "        mutated = False\n",
        "        if original_op == ast.Gt: node.ops[0] = ast.LtE(); mutated = True\n",
        "        elif original_op == ast.Lt: node.ops[0] = ast.GtE(); mutated = True\n",
        "        elif original_op == ast.GtE: node.ops[0] = ast.Lt(); mutated = True\n",
        "        elif original_op == ast.LtE: node.ops[0] = ast.Gt(); mutated = True\n",
        "        elif original_op == ast.Eq: node.ops[0] = ast.NotEq(); mutated = True\n",
        "        elif original_op == ast.NotEq: node.ops[0] = ast.Eq(); mutated = True\n",
        "        elif original_op == ast.Is: node.ops[0] = ast.IsNot(); mutated = True\n",
        "        elif original_op == ast.IsNot: node.ops[0] = ast.Is(); mutated = True\n",
        "        return mutated\n",
        "    return False\n",
        "\n",
        "def mutate_arithmetic_operator(node):\n",
        "    \"\"\"Mutates arithmetic operators (e.g., + to -, * to /).\"\"\"\n",
        "    if isinstance(node, ast.BinOp):\n",
        "        original_op = type(node.op)\n",
        "        mutated = False\n",
        "        if original_op == ast.Add: node.op = ast.Sub(); mutated = True\n",
        "        elif original_op == ast.Sub: node.op = ast.Add(); mutated = True\n",
        "        elif original_op == ast.Mult: node.op = ast.Div(); mutated = True\n",
        "        elif original_op == ast.Div: node.op = ast.Mult(); mutated = True\n",
        "        return mutated\n",
        "    return False\n",
        "\n",
        "def mutate_constant_value(node):\n",
        "    \"\"\"Mutates constant numeric values (e.g., 1 to 0, 0 to 1, N to N+1).\"\"\"\n",
        "    if isinstance(node, ast.Constant) and isinstance(node.value, (int, float)):\n",
        "        original_value = node.value\n",
        "        mutated = False\n",
        "        if original_value == 1: node.value = 0; mutated = True\n",
        "        elif original_value == 0: node.value = 1; mutated = True\n",
        "        else: node.value = original_value + 1; mutated = True\n",
        "        return mutated\n",
        "    return False\n",
        "\n",
        "def mutate_logical_operator(node):\n",
        "    \"\"\"Mutates logical operators (e.g., and to or, or to and).\"\"\"\n",
        "    if isinstance(node, ast.BoolOp):\n",
        "        mutated = False\n",
        "        if isinstance(node.op, ast.And): node.op = ast.Or(); mutated = True\n",
        "        elif isinstance(node.op, ast.Or): node.op = ast.And(); mutated = True\n",
        "        return mutated\n",
        "    return False\n",
        "\n",
        "def evaluate_corrected_test_code(function_id, function_code, entry_point, corrected_test_code, correction_llm_name):\n",
        "    \"\"\"Evaluates the corrected test code.\"\"\"\n",
        "    evaluation = {\n",
        "        \"unique_id\": function_id,\n",
        "        \"correction_llm_model\": correction_llm_name,\n",
        "        \"corrected_test_code\": corrected_test_code,\n",
        "        \"corrected_syntactic_validity\": False,\n",
        "        \"corrected_extracted_assertions\": [],\n",
        "        \"corrected_num_assertions\": 0,\n",
        "        \"corrected_execution_passes\": None,\n",
        "        \"corrected_execution_error\": None\n",
        "    }\n",
        "\n",
        "    if not corrected_test_code or not isinstance(corrected_test_code, str) or not corrected_test_code.strip():\n",
        "        evaluation[\"corrected_execution_error\"] = \"Empty or invalid corrected test code.\"\n",
        "        return evaluation\n",
        "\n",
        "    try:\n",
        "        ast.parse(corrected_test_code)\n",
        "        evaluation[\"corrected_syntactic_validity\"] = True\n",
        "    except SyntaxError as e:\n",
        "        evaluation[\"corrected_execution_error\"] = f\"SyntaxError in corrected code: {e}\"\n",
        "        return evaluation\n",
        "\n",
        "    evaluation[\"corrected_extracted_assertions\"] = extract_assertions_with_context(corrected_test_code)\n",
        "    evaluation[\"corrected_num_assertions\"] = len(evaluation[\"corrected_extracted_assertions\"])\n",
        "\n",
        "    if evaluation[\"corrected_syntactic_validity\"] and entry_point and isinstance(function_code, str) and function_code.strip():\n",
        "        safe_function_id = re.sub(r\"[^a-zA-Z0-9_]\", \"\", str(function_id))\n",
        "        safe_llm_name = re.sub(r\"[^a-zA-Z0-9_]\", \"\", correction_llm_name)\n",
        "        temp_module_name = f\"temp_corrected_test_module_{safe_function_id}_{safe_llm_name}\"\n",
        "\n",
        "        full_code_to_execute = function_code + \"\\n\\n\" + corrected_test_code\n",
        "\n",
        "        if not re.search(r\"if __name__ == .__main__.:\", corrected_test_code, re.IGNORECASE) and \"pytest\" not in corrected_test_code.lower():\n",
        "            test_function_calls = []\n",
        "            try:\n",
        "                parsed_tests = ast.parse(corrected_test_code)\n",
        "                for node in parsed_tests.body:\n",
        "                    if isinstance(node, ast.FunctionDef) and node.name.startswith(\"test\"):\n",
        "                        test_function_calls.append(f\"{node.name}()\")\n",
        "                if test_function_calls:\n",
        "                    full_code_to_execute += \"\\n\\n# Auto-added test runner for corrected tests\\n\" + \"\\n\".join(test_function_calls)\n",
        "            except SyntaxError:\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "            with open(f\"{temp_module_name}.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(full_code_to_execute)\n",
        "\n",
        "            import subprocess\n",
        "            process = subprocess.run([\"python\", f\"{temp_module_name}.py\"], capture_output=True, text=True, timeout=20, encoding=\"utf-8\", errors=\"replace\")\n",
        "\n",
        "            if process.returncode == 0:\n",
        "                evaluation[\"corrected_execution_passes\"] = True\n",
        "            else:\n",
        "                evaluation[\"corrected_execution_passes\"] = False\n",
        "                evaluation[\"corrected_execution_error\"] = process.stderr[:1000]\n",
        "        except Exception as e:\n",
        "            evaluation[\"corrected_execution_error\"] = f\"Runtime error: {str(e)[:500]}\"\n",
        "            evaluation[\"corrected_execution_passes\"] = False\n",
        "        finally:\n",
        "            if os.path.exists(f\"{temp_module_name}.py\"):\n",
        "                os.remove(f\"{temp_module_name}.py\")\n",
        "\n",
        "    return evaluation\n",
        "\n",
        "def generate_mutants(original_code_str, num_mutants_per_function=5):\n",
        "    mutants = []\n",
        "    if not original_code_str or not isinstance(original_code_str, str) or not original_code_str.strip():\n",
        "        return mutants\n",
        "    try:\n",
        "        original_tree = ast.parse(original_code_str)\n",
        "    except SyntaxError:\n",
        "        return mutants\n",
        "\n",
        "    generated_mutant_codes = set()\n",
        "    for _ in range(num_mutants_per_function * 10):\n",
        "        if len(mutants) >= num_mutants_per_function:\n",
        "            break\n",
        "\n",
        "        mutant_tree = ast.parse(original_code_str)\n",
        "        nodes_in_mutant_tree = [n for n in ast.walk(mutant_tree)]\n",
        "        random.shuffle(nodes_in_mutant_tree)\n",
        "\n",
        "        applied_mutation = False\n",
        "        for node_to_mutate in nodes_in_mutant_tree:\n",
        "            operators = [mutate_comparison_operator, mutate_arithmetic_operator, mutate_logical_operator,\n",
        "                         mutate_constant_value]\n",
        "            shuffled_operators = random.sample(operators, len(operators))\n",
        "            for operator_func in shuffled_operators:\n",
        "                try:\n",
        "                    if operator_func(node_to_mutate):\n",
        "                        applied_mutation = True\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "            if applied_mutation:\n",
        "                break\n",
        "\n",
        "        if applied_mutation:\n",
        "            try:\n",
        "                mutant_code = ast.unparse(mutant_tree)\n",
        "                if mutant_code != original_code_str and mutant_code not in generated_mutant_codes:\n",
        "                    ast.parse(mutant_code)  # Verify syntax\n",
        "                    mutants.append(mutant_code)\n",
        "                    generated_mutant_codes.add(mutant_code)\n",
        "            except:\n",
        "                pass\n",
        "    return mutants[:num_mutants_per_function]\n",
        "\n",
        "def run_tests_on_mutant(function_entry_point, mutant_code_str, test_code_str, original_function_code_str):\n",
        "    if not test_code_str or not isinstance(test_code_str, str) or not test_code_str.strip() or \\\n",
        "       not mutant_code_str or not isinstance(mutant_code_str, str) or not mutant_code_str.strip():\n",
        "        return False\n",
        "\n",
        "    full_script_content = mutant_code_str + \"\\n\\n\" + test_code_str\n",
        "\n",
        "    if not re.search(r\"if __name__ == .__main__.:\", test_code_str, re.IGNORECASE) and \"pytest\" not in test_code_str.lower():\n",
        "        test_function_calls = []\n",
        "        try:\n",
        "            parsed_tests = ast.parse(test_code_str)\n",
        "            for node in parsed_tests.body:\n",
        "                if isinstance(node, ast.FunctionDef) and node.name.startswith(\"test\"):\n",
        "                    test_function_calls.append(f\"{node.name}()\")\n",
        "            if test_function_calls:\n",
        "                full_script_content += \"\\n\\n# Auto-added test runner\\n\" + \"\\n\".join(test_function_calls)\n",
        "        except SyntaxError:\n",
        "            pass\n",
        "\n",
        "    safe_entry_point = re.sub(r\"[^a-zA-Z0-9_]\", \"\", str(function_entry_point))\n",
        "    temp_script_path = f\"temp_mutant_test_runner_{safe_entry_point}.py\"\n",
        "\n",
        "    try:\n",
        "        with open(temp_script_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(full_script_content)\n",
        "\n",
        "        process = subprocess.run([sys.executable, temp_script_path], capture_output=True, text=True, timeout=20)\n",
        "        return process.returncode != 0  # Killed if tests fail\n",
        "    except:\n",
        "        return False\n",
        "    finally:\n",
        "        if os.path.exists(temp_script_path):\n",
        "            os.remove(temp_script_path)"
      ],
      "metadata": {
        "id": "jk_GIQTd1P5z"
      },
      "id": "jk_GIQTd1P5z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_assertion_correction_effectiveness(BASE_DRIVE_PATH):\n",
        "    \"\"\"Evaluate the effectiveness of assertion corrections by re-running mutation testing.\"\"\"\n",
        "    print(\"\\n--- Evaluating Assertion Correction Effectiveness ---\")\n",
        "\n",
        "    correction_summary_path = os.path.join(BASE_DRIVE_PATH, \"assertion_correction_results\", \"advanced_correction_summary.csv\")\n",
        "    if not os.path.exists(correction_summary_path):\n",
        "        print(f\"Correction summary not found at {correction_summary_path}. Run assertion correction first.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        correction_df = pd.read_csv(correction_summary_path)\n",
        "        print(f\"Loaded {len(correction_df)} corrected test cases for evaluation.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading correction summary: {e}\")\n",
        "        return\n",
        "\n",
        "    # Filter to only include syntactically valid corrections\n",
        "    valid_corrections = correction_df[correction_df[\"corrected_syntactic_validity\"] == True].copy()\n",
        "    print(f\"Found {len(valid_corrections)} syntactically valid corrections to evaluate.\")\n",
        "\n",
        "    if valid_corrections.empty:\n",
        "        print(\"No valid corrections to evaluate.\")\n",
        "        return\n",
        "\n",
        "    # For each corrected test, re-run mutation testing\n",
        "    evaluation_results = []\n",
        "\n",
        "    for index, row in valid_corrections.iterrows():\n",
        "        unique_id = row[\"unique_id\"]\n",
        "        function_code = str(row[\"function_code\"])\n",
        "        corrected_test_code = str(row[\"corrected_test_code\"])\n",
        "        entry_point = row.get(\"entry_point\", \"\")\n",
        "        original_mutation_score = row.get(\"original_mutation_score\", 0)\n",
        "\n",
        "        if pd.isna(function_code) or pd.isna(corrected_test_code) or pd.isna(entry_point):\n",
        "            print(f\"Skipping {unique_id} due to missing data.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nRe-evaluating function {unique_id} with corrected assertions...\")\n",
        "\n",
        "        # Generate identical mutants as used in the original evaluation\n",
        "        num_mutants = 7  # Use same number for fair comparison\n",
        "        try:\n",
        "            mutants = generate_mutants(function_code, num_mutants_per_function=num_mutants)\n",
        "        except:\n",
        "            print(f\"Error generating mutants for {unique_id}, skipping.\")\n",
        "            continue\n",
        "\n",
        "        if not mutants:\n",
        "            print(f\"No mutants generated for {unique_id}, skipping evaluation.\")\n",
        "            continue\n",
        "\n",
        "        # Test each mutant with the corrected test code\n",
        "        mutants_killed = 0\n",
        "        for i, mutant_code in enumerate(mutants):\n",
        "            try:\n",
        "                killed = run_tests_on_mutant(entry_point, mutant_code, corrected_test_code, function_code)\n",
        "                if killed:\n",
        "                    mutants_killed += 1\n",
        "            except:\n",
        "                print(f\"Error testing mutant {i} for {unique_id}\")\n",
        "\n",
        "        # Calculate the new mutation score\n",
        "        new_mutation_score = (mutants_killed / len(mutants)) * 100 if mutants else 0\n",
        "\n",
        "        # Calculate improvement\n",
        "        if pd.isna(original_mutation_score):\n",
        "            improvement = \"Unknown (original score not available)\"\n",
        "            percent_improvement = 0\n",
        "        else:\n",
        "            improvement = new_mutation_score - original_mutation_score\n",
        "            percent_improvement = (improvement / original_mutation_score * 100) if original_mutation_score > 0 else 0\n",
        "\n",
        "        print(f\"Original score: {original_mutation_score:.2f}%, New score: {new_mutation_score:.2f}%, Improvement: {improvement:.2f} points\")\n",
        "\n",
        "        evaluation_results.append({\n",
        "            \"unique_id\": unique_id,\n",
        "            \"original_mutation_score\": original_mutation_score,\n",
        "            \"corrected_mutation_score\": new_mutation_score,\n",
        "            \"absolute_improvement\": improvement,\n",
        "            \"percent_improvement\": percent_improvement,\n",
        "            \"original_assertions\": row.get(\"original_num_assertions\"),\n",
        "            \"corrected_assertions\": row.get(\"corrected_num_assertions\"),\n",
        "            \"mutants_tested\": len(mutants),\n",
        "            \"mutants_killed\": mutants_killed\n",
        "        })\n",
        "\n",
        "    # Save and display results\n",
        "    if evaluation_results:\n",
        "        eval_df = pd.DataFrame(evaluation_results)\n",
        "        output_path = os.path.join(BASE_DRIVE_PATH, \"assertion_correction_results\", \"correction_effectiveness.csv\")\n",
        "        eval_df.to_csv(output_path, index=False)\n",
        "\n",
        "        print(\"\\n--- Assertion Correction Effectiveness Summary ---\")\n",
        "        print(f\"Average original mutation score: {eval_df['original_mutation_score'].mean():.2f}%\")\n",
        "        print(f\"Average corrected mutation score: {eval_df['corrected_mutation_score'].mean():.2f}%\")\n",
        "        print(f\"Average absolute improvement: {eval_df['absolute_improvement'].mean():.2f} percentage points\")\n",
        "        print(f\"Average percent improvement: {eval_df['percent_improvement'].mean():.2f}%\")\n",
        "        print(f\"Corrections with positive improvement: {len(eval_df[eval_df['absolute_improvement'] > 0])}/{len(eval_df)} ({len(eval_df[eval_df['absolute_improvement'] > 0])/len(eval_df)*100:.2f}%)\")\n",
        "\n",
        "        display(eval_df.head())\n",
        "\n",
        "        # Create summary by original LLM\n",
        "        if 'original_llm_model' in correction_df.columns:\n",
        "            merged_eval = pd.merge(eval_df, correction_df[['unique_id', 'original_llm_model']], on='unique_id', how='left')\n",
        "            model_summary = merged_eval.groupby('original_llm_model').agg({\n",
        "                'original_mutation_score': 'mean',\n",
        "                'corrected_mutation_score': 'mean',\n",
        "                'absolute_improvement': 'mean'\n",
        "            }).round(2).reset_index()\n",
        "\n",
        "            print(\"\\nImprovement by original LLM model:\")\n",
        "            display(model_summary)\n",
        "    else:\n",
        "        print(\"No mutation testing results to report.\")\n",
        "\n",
        "    print(\"\\n--- Assertion Correction Effectiveness Evaluation Completed ---\")"
      ],
      "metadata": {
        "id": "dP4jPmIQ1SlB"
      },
      "id": "dP4jPmIQ1SlB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_advanced_assertion_correction_pipeline(BASE_DRIVE_PATH, correction_llm_pipeline=None):\n",
        "    \"\"\"\n",
        "    Implements advanced assertion correction pipeline using specialized LLM prompts,\n",
        "    pattern-based refinement, and semantic refinement as described in the methodology.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Advanced Assertion Correction Pipeline ---\")\n",
        "\n",
        "    mutation_results_path = os.path.join(BASE_DRIVE_PATH, \"mutation_testing_results\", \"mutation_testing_summary.csv\")\n",
        "    llm_tests_path = os.path.join(BASE_DRIVE_PATH, \"llm_generated_tests\", \"llm_generated_tests_output.csv\")\n",
        "    baseline_eval_path = os.path.join(BASE_DRIVE_PATH, \"llm_generated_tests\", \"baseline_evaluation_summary.csv\")\n",
        "\n",
        "    if not os.path.exists(mutation_results_path) or not os.path.exists(llm_tests_path) or not os.path.exists(baseline_eval_path):\n",
        "        print(\"Required input files (mutation results, LLM tests, or baseline eval) not found. Run previous steps.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        mutation_df = pd.read_csv(mutation_results_path)\n",
        "        llm_tests_df = pd.read_csv(llm_tests_path)\n",
        "        baseline_df = pd.read_csv(baseline_eval_path)\n",
        "\n",
        "        # Merge dataframes to get comprehensive view of each test case\n",
        "        merged_df = pd.merge(llm_tests_df, baseline_df[[\"unique_id\", \"llm_model\", \"execution_passes\", \"syntactic_validity\", \"num_assertions\"]],\n",
        "                             on=[\"unique_id\", \"llm_model\"], how=\"left\", suffixes=(\"\", \"_baseline\"))\n",
        "        merged_df = pd.merge(merged_df, mutation_df[[\"unique_id\", \"llm_model\", \"mutation_score\", \"mutants_killed\", \"total_mutants_generated\"]],\n",
        "                             on=[\"unique_id\", \"llm_model\"], how=\"left\", suffixes=(\"\", \"_mutation\"))\n",
        "        print(f\"Loaded and merged all relevant data: {len(merged_df)} entries.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or merging data: {e}\")\n",
        "        return\n",
        "\n",
        "    # Use comprehensive criteria for selecting candidates for correction\n",
        "    correction_threshold = 75.0\n",
        "    candidates_for_correction = merged_df[\n",
        "        # Either low mutation score\n",
        "        (merged_df[\"mutation_score\"].fillna(101) < correction_threshold) |\n",
        "        # Or execution failures despite valid syntax\n",
        "        ((merged_df.get(\"execution_passes_baseline\") == False) & (merged_df.get(\"syntactic_validity_baseline\") == True)) |\n",
        "        # Or very few assertions\n",
        "        (merged_df.get(\"num_assertions\", pd.Series([5] * len(merged_df))) < 3)\n",
        "    ].copy()\n",
        "\n",
        "    print(f\"Identified {len(candidates_for_correction)} test suites as candidates for correction.\")\n",
        "    if candidates_for_correction.empty:\n",
        "        print(\"No candidates for assertion correction based on current criteria.\")\n",
        "        return\n",
        "\n",
        "    if correction_llm_pipeline is None:\n",
        "        print(\"Correction LLM pipeline not provided, loading DeepSeek-Coder-1.3B-instruct with quantization.\")\n",
        "        # Implement a function to load your LLM here\n",
        "        # For example:\n",
        "        # correction_llm_pipeline = load_correction_llm()\n",
        "        if not correction_llm_pipeline:\n",
        "            print(\"Failed to load LLM for correction. Aborting.\")\n",
        "            return\n",
        "    else:\n",
        "        model_name = correction_llm_pipeline.model.config._name_or_path if hasattr(correction_llm_pipeline.model, \"config\") else \"Passed LLM\"\n",
        "        print(f\"Using provided LLM for correction: {model_name}\")\n",
        "\n",
        "    correction_results_list = []\n",
        "\n",
        "    # Process candidates for correction\n",
        "    for index, row in candidates_for_correction.iterrows():\n",
        "        unique_id = row[\"unique_id\"]\n",
        "        function_code = str(row[\"function_code\"])\n",
        "        original_test_code = str(row[\"generated_test_code\"])\n",
        "        llm_model_source = row[\"llm_model\"]\n",
        "        entry_point = str(row.get(\"entry_point\", \"\"))\n",
        "\n",
        "        # Prepare mutation results data structure for the correction process\n",
        "        mutation_results_data = {\n",
        "            \"mutation_score\": row.get(\"mutation_score\"),\n",
        "            \"mutants_killed\": row.get(\"mutants_killed\", 0),\n",
        "            \"total_mutants_generated\": row.get(\"total_mutants_generated\", 0),\n",
        "            \"survived_mutations\": analyze_survived_mutations(function_code, original_test_code)\n",
        "        }\n",
        "\n",
        "        print(f\"\\nRunning advanced assertion correction for function {unique_id} (original tests by {llm_model_source})...\")\n",
        "\n",
        "        # Use advanced assertion correction function\n",
        "        corrected_test_code = advanced_assertion_correction(\n",
        "            correction_llm_pipeline,\n",
        "            function_code,\n",
        "            original_test_code,\n",
        "            mutation_results_data,\n",
        "            enhanced_prompt_template\n",
        "        )\n",
        "\n",
        "        # Evaluate the corrected test code\n",
        "        if corrected_test_code:\n",
        "            corrected_evaluation = evaluate_corrected_test_code(\n",
        "                unique_id, function_code, entry_point, corrected_test_code,\n",
        "                correction_llm_pipeline.model.config._name_or_path\n",
        "            )\n",
        "\n",
        "            # Store the results\n",
        "            correction_results_list.append({\n",
        "                \"unique_id\": unique_id,\n",
        "                \"original_llm_model\": llm_model_source,\n",
        "                \"correction_llm_model\": correction_llm_pipeline.model.config._name_or_path,\n",
        "                \"function_code\": function_code,\n",
        "                \"original_test_code\": original_test_code,\n",
        "                \"corrected_test_code\": corrected_test_code,\n",
        "                \"original_mutation_score\": row.get(\"mutation_score\"),\n",
        "                \"original_num_assertions\": row.get(\"num_assertions\"),\n",
        "                \"corrected_num_assertions\": corrected_evaluation.get(\"corrected_num_assertions\"),\n",
        "                \"corrected_syntactic_validity\": corrected_evaluation.get(\"corrected_syntactic_validity\"),\n",
        "                \"corrected_execution_passes\": corrected_evaluation.get(\"corrected_execution_passes\"),\n",
        "                \"corrected_execution_error\": corrected_evaluation.get(\"corrected_execution_error\")\n",
        "            })\n",
        "        else:\n",
        "            print(f\"Correction failed for {unique_id}\")\n",
        "\n",
        "        # Small delay between operations\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    if correction_results_list:\n",
        "        # Save the correction results\n",
        "        correction_results_df = pd.DataFrame(correction_results_list)\n",
        "        output_dir = os.path.join(BASE_DRIVE_PATH, \"assertion_correction_results\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        correction_results_path = os.path.join(output_dir, \"advanced_correction_summary.csv\")\n",
        "        correction_results_df.to_csv(correction_results_path, index=False)\n",
        "\n",
        "        print(f\"\\nAdvanced assertion correction results saved to {correction_results_path}\")\n",
        "        print(\"\\nSummary of corrections:\")\n",
        "        print(f\"- Total corrections attempted: {len(correction_results_df)}\")\n",
        "        print(f\"- Syntactically valid: {correction_results_df['corrected_syntactic_validity'].sum()}\")\n",
        "        if 'corrected_execution_passes' in correction_results_df.columns:\n",
        "            passes = correction_results_df['corrected_execution_passes'].sum()\n",
        "            print(f\"- Execution passes: {passes} ({passes/len(correction_results_df)*100:.1f}%)\")\n",
        "        print(f\"- Average original assertions: {correction_results_df['original_num_assertions'].mean():.2f}\")\n",
        "        print(f\"- Average corrected assertions: {correction_results_df['corrected_num_assertions'].mean():.2f}\")\n",
        "\n",
        "        if not correction_results_df.empty:\n",
        "            display(correction_results_df[['unique_id', 'original_llm_model', 'original_num_assertions',\n",
        "                                        'corrected_num_assertions', 'corrected_syntactic_validity',\n",
        "                                        'corrected_execution_passes']].head())\n",
        "    else:\n",
        "        print(\"No successful corrections to save.\")\n",
        "\n",
        "    print(\"\\n--- Advanced Assertion Correction Pipeline Completed ---\")"
      ],
      "metadata": {
        "id": "qWx9-QJe1VL-"
      },
      "id": "qWx9-QJe1VL-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "\n",
        "# Define your base drive path where data is stored\n",
        "BASE_DRIVE_PATH = \"/content/drive/MyDrive/Colab Notebooks/Assertion-Accuracy-Research/Data\"\n",
        "\n",
        "# Load a model for assertion correction\n",
        "from transformers import pipeline\n",
        "correction_llm = pipeline(\n",
        "     \"text-generation\",\n",
        "     model=\"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
        "     device_map=\"auto\"\n",
        " )\n",
        "\n",
        "# Run the correction pipeline\n",
        "run_advanced_assertion_correction_pipeline(BASE_DRIVE_PATH, correction_llm)\n",
        "\n",
        "# Evaluate effectiveness\n",
        "evaluate_assertion_correction_effectiveness(BASE_DRIVE_PATH)"
      ],
      "metadata": {
        "id": "ADUbuFjJb1Nn"
      },
      "id": "ADUbuFjJb1Nn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset for fine-tuning assertion correction model\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "def prepare_assertion_correction_dataset(BASE_DRIVE_PATH, tokenizer, max_length=1024):\n",
        "  \"\"\"\n",
        "  Prepare dataset for fine-tuning assertion correction models using existing corrections.\n",
        "\n",
        "  Args:\n",
        "    BASE_DRIVE_PATH: Base path where correction data is stored\n",
        "    tokenizer: The tokenizer for the model to be fine-tuned\n",
        "    max_length: Maximum sequence length for tokenization\n",
        "\n",
        "  Returns:\n",
        "    train_dataset, eval_dataset: PyTorch datasets ready for fine-tuning\n",
        "  \"\"\"\n",
        "  print(\"\\n--- Preparing Assertion Correction Dataset for Fine-tuning ---\")\n",
        "\n",
        "  # Check for correction data\n",
        "  correction_summary_path = os.path.join(BASE_DRIVE_PATH, \"assertion_correction_results\", \"advanced_correction_summary.csv\")\n",
        "  if not os.path.exists(correction_summary_path):\n",
        "    print(f\"Correction summary not found at {correction_summary_path}\")\n",
        "    return None, None\n",
        "\n",
        "  # Load correction data\n",
        "  try:\n",
        "    df = pd.read_csv(correction_summary_path)\n",
        "    print(f\"Loaded {len(df)} correction examples\")\n",
        "  except Exception as e:\n",
        "    print(f\"Error loading correction data: {e}\")\n",
        "    return None, None\n",
        "\n",
        "  # Filter to include only successful corrections (syntactically valid and passes execution)\n",
        "  successful_df = df[\n",
        "      (df[\"corrected_syntactic_validity\"] == True) &\n",
        "      (df[\"corrected_execution_passes\"] == True) &\n",
        "      (df[\"corrected_num_assertions\"] > df[\"original_num_assertions\"])\n",
        "  ].copy()\n",
        "\n",
        "  print(f\"Found {len(successful_df)} successful corrections for training\")\n",
        "\n",
        "  if len(successful_df) < 10: # Need minimum examples for meaningful training\n",
        "    print(f\"Insufficient successful corrections ({len(successful_df)}) for fine-tuning. Need at least 10.\")\n",
        "    return None, None\n",
        "\n",
        "  # Prepare instruction format for each example\n",
        "  training_examples = []\n",
        "  for idx, row in successful_df.iterrows():\n",
        "    # Format input as instruction\n",
        "    # Corrected the reference to original_test_code from row[' '] to row['original_test_code']\n",
        "    # Assuming 'original_test_code' is the correct column name for the original test code.\n",
        "    # If it's different, please adjust.\n",
        "    instruction = f\"\"\"Function to test:\n",
        "```python\n",
        "{row['function_code']}\n",
        "```\n",
        "Original test code with weak assertions:\n",
        "```python\n",
        "{row['original_test_code']}\n",
        "```\n",
        "Improve the assertions in this test code to be more comprehensive and accurate.\"\"\"\n",
        "\n",
        "    # The expected output is the corrected test code\n",
        "    response = row['corrected_test_code']\n",
        "\n",
        "    # Create a training example\n",
        "    training_examples.append({\n",
        "        \"instruction\": instruction,\n",
        "        \"input\": \"\", # Input can be empty as instruction contains everything\n",
        "        \"output\": response\n",
        "    })\n",
        "\n",
        "  # Split into training and validation sets\n",
        "  train_examples, eval_examples = train_test_split(training_examples, test_size=0.2, random_state=42)\n",
        "\n",
        "  print(f\"Created {len(train_examples)} training examples and {len(eval_examples)} validation examples\")\n",
        "\n",
        "  # Convert to HF datasets\n",
        "  train_dataset = Dataset.from_pandas(pd.DataFrame(train_examples))\n",
        "  eval_dataset = Dataset.from_pandas(pd.DataFrame(eval_examples))\n",
        "\n",
        "  # Tokenize the datasets\n",
        "  def tokenize_function(examples):\n",
        "    # Combine instruction and input for the prompt\n",
        "    if examples[\"input\"]:\n",
        "      prompts = [f\"<s>[INST] {instruction} {input} [/INST]\"\n",
        "                 for instruction, input in zip(examples[\"instruction\"], examples[\"input\"])]\n",
        "    else:\n",
        "      prompts = [f\"<s>[INST] {instruction} [/INST]\"\n",
        "                 for instruction in examples[\"instruction\"]]\n",
        "\n",
        "    tokenized_prompts = tokenizer(prompts, truncation=True, max_length=max_length // 2)\n",
        "\n",
        "    # Tokenize the responses/outputs\n",
        "    tokenized_outputs = tokenizer(examples[\"output\"], truncation=True, max_length=max_length // 2)\n",
        "\n",
        "    # Combine them into input_ids and attention_mask\n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    labels = []\n",
        "\n",
        "    for i in range(len(tokenized_prompts[\"input_ids\"])):\n",
        "      # Get the prompt tokens (with special tokens)\n",
        "      prompt_input_ids = tokenized_prompts[\"input_ids\"][i]\n",
        "      prompt_attention_mask = tokenized_prompts[\"attention_mask\"][i]\n",
        "\n",
        "      # Get the output/response tokens\n",
        "      output_input_ids = tokenized_outputs[\"input_ids\"][i]\n",
        "      # output_attention_mask = tokenized_outputs[\"attention_mask\"][i] # This was not used previously, can be added if needed\n",
        "\n",
        "      # Create combined tokens - the prompt followed by the expected output\n",
        "      combined_input_ids = prompt_input_ids + output_input_ids\n",
        "      combined_attention_mask = prompt_attention_mask + ([1] * len(output_input_ids)) # Ensure attention mask covers output\n",
        "\n",
        "      # Create labels: -100 for prompt tokens (they're ignored in loss calculation)\n",
        "      # and actual token IDs for response tokens\n",
        "      combined_labels = [-100] * len(prompt_input_ids) + output_input_ids\n",
        "\n",
        "      # Ensure we don't exceed max_length\n",
        "      if len(combined_input_ids) > max_length:\n",
        "        combined_input_ids = combined_input_ids[:max_length]\n",
        "        combined_attention_mask = combined_attention_mask[:max_length]\n",
        "        combined_labels = combined_labels[:max_length]\n",
        "\n",
        "      input_ids.append(combined_input_ids)\n",
        "      attention_mask.append(combined_attention_mask)\n",
        "      labels.append(combined_labels)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels\n",
        "    }\n",
        "\n",
        "  # Apply tokenization\n",
        "  train_tokenized = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n",
        "  eval_tokenized = eval_dataset.map(tokenize_function, batched=True, remove_columns=eval_dataset.column_names)\n",
        "\n",
        "  # Convert to PyTorch tensors\n",
        "  train_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "  eval_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "  print(\"Dataset preparation complete\")\n",
        "\n",
        "  return train_tokenized, eval_tokenized"
      ],
      "metadata": {
        "id": "2xxIEj0E1lX5"
      },
      "id": "2xxIEj0E1lX5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure LoRA for Fine-tuning\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftConfig, PeftModel\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch # Ensure torch is imported if not already\n",
        "\n",
        "def setup_lora_model_for_finetuning(\n",
        "    base_model_name=\"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
        "    use_quantization=True,\n",
        "    lora_r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05\n",
        "):\n",
        "  \"\"\"\n",
        "  Set up a model with LoRA adapters for fine-tuning.\n",
        "\n",
        "  Args:\n",
        "    base_model_name: Name or path of the base model\n",
        "    use_quantization: Whether to use 4-bit quantization for memory efficiency\n",
        "    lora_r: LoRA attention dimension\n",
        "    lora_alpha: Alpha parameter for LoRA scaling\n",
        "    lora_dropout: Dropout probability for LoRA layers\n",
        "\n",
        "  Returns:\n",
        "    model: The model with LoRA adapters\n",
        "    tokenizer: The model's tokenizer (Note: tokenizer is loaded but not returned in original, corrected to return)\n",
        "  \"\"\"\n",
        "  print(f\"\\n--- Setting up {base_model_name} with LoRA adapters ---\")\n",
        "\n",
        "  # Load tokenizer\n",
        "  tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "  tokenizer.pad_token = tokenizer.eos_token # Set pad token\n",
        "\n",
        "  # Configure quantization if enabled\n",
        "  quantization_config = None # Initialize to None\n",
        "  if use_quantization:\n",
        "    print(\"Using 4-bit quantization for memory-efficient training\")\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True\n",
        "    )\n",
        "  else:\n",
        "    print(\"Using full precision for training\")\n",
        "\n",
        "  # Load the model with quantization if enabled\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "      base_model_name,\n",
        "      quantization_config=quantization_config,\n",
        "      device_map=\"auto\" # Automatically maps model to available devices (e.g., GPU)\n",
        "  )\n",
        "\n",
        "  # Configure LoRA\n",
        "  print(f\"Configuring LoRA with r={lora_r}, alpha={lora_alpha}, dropout={lora_dropout}\")\n",
        "  lora_config = LoraConfig(\n",
        "      r=lora_r,\n",
        "      lora_alpha=lora_alpha,\n",
        "      lora_dropout=lora_dropout,\n",
        "      bias=\"none\",\n",
        "      task_type=TaskType.CAUSAL_LM,\n",
        "      target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"] # Common target modules for transformer models\n",
        "  )\n",
        "\n",
        "  # Wrap the model with LoRA adapters\n",
        "  model = get_peft_model(model, lora_config)\n",
        "\n",
        "  # Print trainable vs total parameters\n",
        "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  total_params = sum(p.numel() for p in model.parameters())\n",
        "  print(f\"Trainable parameters: {trainable_params} ({trainable_params / total_params * 100:.2f}% of total)\")\n",
        "\n",
        "  model.print_trainable_parameters() # Method to display trainable parameters summary\n",
        "\n",
        "  return model, tokenizer # Corrected to return tokenizer as well"
      ],
      "metadata": {
        "id": "GL1vR4y_1oZx"
      },
      "id": "GL1vR4y_1oZx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop for Fine-tuning\n",
        "!pip install -q evaluate rouge_score\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import os\n",
        "import torch\n",
        "import evaluate # Make sure evaluate (for rouge) is imported\n",
        "\n",
        "def train_assertion_correction_model(\n",
        "    BASE_DRIVE_PATH,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    train_dataset,\n",
        "    eval_dataset,\n",
        "    output_dir_name=\"assertion-correction-model-v2\", # Changed default to avoid conflict if run multiple times\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4,\n",
        "    warmup_ratio=0.03\n",
        "):\n",
        "  \"\"\"\n",
        "  Fine-tune a model with LoRA adapters on assertion correction tasks.\n",
        "\n",
        "  Args:\n",
        "    BASE_DRIVE_PATH: Base directory for saving models and logs\n",
        "    model: The model with LoRA adapters\n",
        "    tokenizer: The model's tokenizer\n",
        "    train_dataset: Training dataset\n",
        "    eval_dataset: Evaluation dataset\n",
        "    output_dir_name: Subdirectory name for saving model checkpoints\n",
        "    num_train_epochs: Number of training epochs\n",
        "    per_device_train_batch_size: Batch size per device\n",
        "    gradient_accumulation_steps: Steps for gradient accumulation\n",
        "    learning_rate: Learning rate for training\n",
        "    warmup_ratio: Portion of training steps for learning rate warmup\n",
        "\n",
        "  Returns:\n",
        "    trainer: The trainer object after training\n",
        "  \"\"\"\n",
        "  print(\"\\n--- Starting Fine-tuning for Assertion Correction ---\")\n",
        "\n",
        "  # Set up output directory\n",
        "  output_dir = os.path.join(BASE_DRIVE_PATH, output_dir_name)\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "  # Load metric for evaluation\n",
        "  # Ensure the `evaluate` library is installed: pip install evaluate rouge_score\n",
        "  try:\n",
        "    rouge_metric = evaluate.load(\"rouge\")\n",
        "  except Exception as e:\n",
        "    print(f\"Error loading rouge metric: {e}. Please ensure 'evaluate' and 'rouge_score' are installed.\")\n",
        "    print(\"You can install them with: !pip install evaluate rouge_score\")\n",
        "    return None\n",
        "\n",
        "  def compute_metrics(eval_preds):\n",
        "    predictions, labels = eval_preds\n",
        "\n",
        "    # Decode predictions\n",
        "    # Ensure pad_token_id is handled if it was set during tokenization for generation\n",
        "    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    # Decode reference labels, filtering out -100 values (used for masking prompt tokens)\n",
        "    # Replace -100 with pad_token_id before decoding, or ensure skip_special_tokens handles it.\n",
        "    labels = [[token_id if token_id != -100 else tokenizer.pad_token_id for token_id in label_example] for label_example in labels]\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Compute ROUGE scores\n",
        "    result = rouge_metric.compute(predictions=decoded_predictions, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "    # Log a sample prediction for inspection\n",
        "    sample_idx = 0\n",
        "    if len(decoded_labels) > sample_idx and len(decoded_predictions) > sample_idx:\n",
        "        print(\"\\nSample prediction comparison:\")\n",
        "        print(f\"Reference:\\n{decoded_labels[sample_idx][:200]}...\")\n",
        "        print(f\"Prediction:\\n{decoded_predictions[sample_idx][:200]}...\")\n",
        "    else:\n",
        "        print(\"\\nNot enough samples to display prediction comparison.\")\n",
        "\n",
        "    # Add precision, recall, fmeasure for ROUGE scores to the logs\n",
        "    prediction_lens = [len(pred.split()) for pred in decoded_predictions]\n",
        "    result[\"gen_len\"] = sum(prediction_lens) / len(prediction_lens)\n",
        "    return {key: value for key, value in result.items()}\n",
        "\n",
        "  # Configure training arguments\n",
        "  training_args = TrainingArguments(\n",
        "      output_dir=output_dir,\n",
        "      num_train_epochs=num_train_epochs,\n",
        "      per_device_train_batch_size=per_device_train_batch_size,\n",
        "      per_device_eval_batch_size=per_device_train_batch_size, # Corrected from train batch size\n",
        "      gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "      evaluation_strategy=\"epoch\",\n",
        "      save_strategy=\"epoch\",\n",
        "      logging_dir=os.path.join(output_dir, \"logs\"), # Ensure logs directory is created\n",
        "      logging_steps=10,\n",
        "      learning_rate=learning_rate,\n",
        "      weight_decay=0.01,\n",
        "      warmup_ratio=warmup_ratio,\n",
        "      lr_scheduler_type=\"cosine\",\n",
        "      load_best_model_at_end=True,\n",
        "      metric_for_best_model=\"rouge1\", # Make sure this matches a key in compute_metrics output\n",
        "      push_to_hub=False,\n",
        "      report_to=\"tensorboard\", # Or \"wandb\" or \"none\"\n",
        "      remove_unused_columns=False # Important for proper dataset handling with custom tokenization\n",
        "  )\n",
        "\n",
        "  # Configure the trainer\n",
        "  trainer = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=train_dataset,\n",
        "      eval_dataset=eval_dataset,\n",
        "      compute_metrics=compute_metrics,\n",
        "      tokenizer=tokenizer # Pass tokenizer for decoding in compute_metrics\n",
        "  )\n",
        "\n",
        "  # Train the model\n",
        "  print(f\"Starting training for {num_train_epochs} epochs...\")\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache() # Clear GPU memory if CUDA is available\n",
        "\n",
        "  try:\n",
        "    trainer.train()\n",
        "  except Exception as e:\n",
        "    print(f\"Error during training: {e}\")\n",
        "    return None\n",
        "\n",
        "  # Save the final model\n",
        "  final_model_path = os.path.join(output_dir, \"final_model_checkpoint\") # More descriptive name\n",
        "  trainer.save_model(final_model_path)\n",
        "  print(f\"Training complete. Model saved to {final_model_path}\")\n",
        "\n",
        "  return trainer"
      ],
      "metadata": {
        "id": "M5NPjqAPhz22"
      },
      "id": "M5NPjqAPhz22",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and Use Fine-tuned Model\n",
        "\n",
        "from peft import PeftConfig, PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "import os\n",
        "import torch\n",
        "\n",
        "def load_finetuned_assertion_model(BASE_DRIVE_PATH, model_dir_name=\"assertion-correction-model-v2\"):\n",
        "  \"\"\"\n",
        "  Load the fine-tuned assertion correction model with LoRA adapters.\n",
        "\n",
        "  Args:\n",
        "    BASE_DRIVE_PATH: Base directory where models are stored\n",
        "    model_dir_name: Name of the model subdirectory (should match the one used in training)\n",
        "\n",
        "  Returns:\n",
        "    pipe: A text generation pipeline using the fine-tuned model, or None if loading fails.\n",
        "  \"\"\"\n",
        "  print(\"\\n--- Loading Fine-tuned Assertion Correction Model ---\")\n",
        "\n",
        "  # Path to the final saved model checkpoint from training\n",
        "  model_path = os.path.join(BASE_DRIVE_PATH, model_dir_name, \"final_model_checkpoint\")\n",
        "\n",
        "  if not os.path.exists(model_path):\n",
        "    print(f\"Model not found at {model_path}. Please ensure training was successful and the path is correct.\")\n",
        "    return None\n",
        "\n",
        "  try:\n",
        "    # Load the PEFT configuration from the saved model directory\n",
        "    peft_config = PeftConfig.from_pretrained(model_path)\n",
        "\n",
        "    print(f\"Loading base model: {peft_config.base_model_name_or_path}\")\n",
        "\n",
        "    # Configure quantization (should match training configuration for consistency)\n",
        "    # Assuming use_quantization was True during training for this example.\n",
        "    # If not, this part should be conditional or removed.\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True\n",
        "    )\n",
        "\n",
        "    # Load base model with the same quantization used during training\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        peft_config.base_model_name_or_path,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\" # Automatically map to available device\n",
        "    )\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)\n",
        "    tokenizer.pad_token = tokenizer.eos_token # Ensure pad token is set\n",
        "\n",
        "    # Load LoRA adapters onto the base model\n",
        "    print(\"Loading LoRA adapters...\")\n",
        "    model = PeftModel.from_pretrained(base_model, model_path)\n",
        "    model = model.merge_and_unload() # Optional: merge LoRA weights for faster inference if not continuing training\n",
        "    model.eval() # Set model to evaluation mode\n",
        "\n",
        "    # Create text generation pipeline\n",
        "    # Ensure the task is appropriate, e.g., \"text-generation\" for causal LMs\n",
        "    pipe = pipeline(\n",
        "        task=\"text-generation\", # or \"summarization\", \"translation\" etc. based on model and task\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=2048, # Adjust as needed for expected output length\n",
        "        do_sample=True,\n",
        "        temperature=0.7, # Controls randomness; lower is more deterministic\n",
        "        top_p=0.95,      # Nucleus sampling: considers a smaller set of high-probability tokens\n",
        "        # pad_token_id=tokenizer.eos_token_id # Important for some models if pad_token is eos_token\n",
        "    )\n",
        "\n",
        "    print(\"Fine-tuned model loaded successfully and pipeline created.\")\n",
        "    return pipe\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Error loading fine-tuned model: {e}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "2CKpwG4YiHd2"
      },
      "id": "2CKpwG4YiHd2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Fine-tuning Pipeline\n",
        "\n",
        "# Ensure the functions from previous cells are defined:\n",
        "# - setup_lora_model_for_finetuning (from colab_cell_02_lora_setup.py)\n",
        "# - prepare_assertion_correction_dataset (from colab_cell_01_dataset_prep.py)\n",
        "# - train_assertion_correction_model (from colab_cell_03_training_loop.py)\n",
        "# - load_finetuned_assertion_model (from colab_cell_04_load_model.py)\n",
        "\n",
        "\n",
        "def run_assertion_model_finetuning_pipeline(\n",
        "    BASE_DRIVE_PATH, # Make sure this path is correctly set in your Colab environment\n",
        "    base_model_name=\"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
        "    use_quantization=True,\n",
        "    output_dir_name_training=\"assertion-correction-model-v2\", # Should match training output\n",
        "    num_epochs_training=3 # Should match training epochs\n",
        "):\n",
        "  \"\"\"\n",
        "  Run the complete pipeline for fine-tuning a model for assertion correction.\n",
        "\n",
        "  Args:\n",
        "    BASE_DRIVE_PATH: Base path where data and models are stored/will be stored.\n",
        "    base_model_name: Name of the base model to fine-tune.\n",
        "    use_quantization: Whether to use quantization for memory efficiency during setup.\n",
        "    output_dir_name_training: The directory name used during training to save the model.\n",
        "    num_epochs_training: The number of epochs used for training.\n",
        "\n",
        "  Returns:\n",
        "    True if fine-tuning and loading were successful, False otherwise.\n",
        "  \"\"\"\n",
        "  print(\"\\n=== Starting Assertion Correction Model Fine-tuning Pipeline ===\")\n",
        "\n",
        "  # Check if BASE_DRIVE_PATH is set\n",
        "  if not BASE_DRIVE_PATH or not os.path.exists(BASE_DRIVE_PATH):\n",
        "      print(f\"Error: BASE_DRIVE_PATH (\\'{BASE_DRIVE_PATH}\\') is not set or does not exist.\")\n",
        "      print(\"Please set it to your Google Drive path where data and models will be stored.\")\n",
        "      print(\"Example: BASE_DRIVE_PATH = \\\"/content/drive/MyDrive/your_project_folder\\\"\")\n",
        "      print(\"If using Google Drive, ensure it is mounted: from google.colab import drive; drive.mount(\\\"/content/drive\\\")\")\n",
        "      return False\n",
        "\n",
        "  try:\n",
        "    # 1. Set up the model with LoRA adapters\n",
        "    print(\"\\nStep 1: Setting up LoRA model...\")\n",
        "    model, tokenizer = setup_lora_model_for_finetuning(\n",
        "        base_model_name=base_model_name,\n",
        "        use_quantization=use_quantization\n",
        "        # lora_r, lora_alpha, lora_dropout will use defaults from the function definition\n",
        "    )\n",
        "\n",
        "    if model is None or tokenizer is None:\n",
        "      print(\"Failed to set up the model with LoRA adapters. Exiting pipeline.\")\n",
        "      return False\n",
        "    print(\"LoRA model setup complete.\")\n",
        "\n",
        "    # 2. Prepare the dataset\n",
        "    print(\"\\nStep 2: Preparing dataset...\")\n",
        "    train_dataset, eval_dataset = prepare_assertion_correction_dataset(\n",
        "        BASE_DRIVE_PATH=BASE_DRIVE_PATH,\n",
        "        tokenizer=tokenizer\n",
        "        # max_length will use default from the function definition\n",
        "    )\n",
        "\n",
        "    if train_dataset is None or eval_dataset is None:\n",
        "      print(\"Failed to prepare the dataset for fine-tuning. Exiting pipeline.\")\n",
        "      return False\n",
        "    print(\"Dataset preparation complete.\")\n",
        "\n",
        "    # 3. Run the fine-tuning\n",
        "    print(\"\\nStep 3: Starting training...\")\n",
        "    trainer = train_assertion_correction_model(\n",
        "        BASE_DRIVE_PATH=BASE_DRIVE_PATH,\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        output_dir_name=output_dir_name_training, # Use the specified output directory\n",
        "        num_train_epochs=num_epochs_training # Use the specified number of epochs\n",
        "        # Other training parameters will use defaults from the function definition\n",
        "    )\n",
        "\n",
        "    if trainer is None:\n",
        "      print(\"Training failed. Exiting pipeline.\")\n",
        "      return False\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "    # 4. Test loading the fine-tuned model\n",
        "    print(\"\\nStep 4: Loading and testing fine-tuned model...\")\n",
        "    test_pipeline = load_finetuned_assertion_model(\n",
        "        BASE_DRIVE_PATH=BASE_DRIVE_PATH,\n",
        "        model_dir_name=output_dir_name_training # Load from the same directory used in training\n",
        "    )\n",
        "\n",
        "    if test_pipeline is None:\n",
        "      print(\"Failed to load the fine-tuned model for testing. Exiting pipeline.\")\n",
        "      return False\n",
        "    print(\"Fine-tuned model loaded successfully for testing.\")\n",
        "\n",
        "    print(\"\\n=== Assertion Correction Model Fine-tuning Pipeline Completed Successfully! ===\")\n",
        "    return True\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred in the fine-tuning pipeline: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    return False\n",
        "\n",
        "# Example of how to run the pipeline in Colab:\n",
        "# Make sure to set your BASE_DRIVE_PATH first.\n",
        "# For example:\n",
        "BASE_DRIVE_PATH = \"/content/drive/MyDrive/Colab Notebooks/Assertion-Accuracy-Research/Data\"\n",
        "# if not os.path.exists(BASE_DRIVE_PATH):\n",
        "#     os.makedirs(BASE_DRIVE_PATH)\n",
        "#\n",
        "# Then you can call:\n",
        "success = run_assertion_model_finetuning_pipeline(BASE_DRIVE_PATH)\n",
        "if success:\n",
        "   print(\"Pipeline ran successfully.\")\n",
        "else:\n",
        "   print(\"Pipeline encountered errors.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLFGC9b1iSYV",
        "outputId": "7fa17264-c851-4270-b6fd-83e3fb2a9f07"
      },
      "id": "nLFGC9b1iSYV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Starting Assertion Correction Model Fine-tuning Pipeline ===\n",
            "\n",
            "Step 1: Setting up LoRA model...\n",
            "\n",
            "--- Setting up deepseek-ai/deepseek-coder-1.3b-instruct with LoRA adapters ---\n",
            "Using 4-bit quantization for memory-efficient training\n",
            "Configuring LoRA with r=8, alpha=32, dropout=0.05\n",
            "Trainable parameters: 3145728 (0.42% of total)\n",
            "trainable params: 3,145,728 || all params: 1,349,617,664 || trainable%: 0.2331\n",
            "LoRA model setup complete.\n",
            "\n",
            "Step 2: Preparing dataset...\n",
            "\n",
            "--- Preparing Assertion Correction Dataset for Fine-tuning ---\n",
            "Loaded 995 correction examples\n",
            "Found 0 successful corrections for training\n",
            "Insufficient successful corrections (0) for fine-tuning. Need at least 10.\n",
            "Failed to prepare the dataset for fine-tuning. Exiting pipeline.\n",
            "Pipeline encountered errors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--end-------------------------"
      ],
      "metadata": {
        "id": "7l1fBKffqrBd"
      },
      "id": "7l1fBKffqrBd"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RUdppO3dTR4W",
        "TdiFNRm4TR4X",
        "-ak3wgkfTR4Y",
        "28u6WECiTR4Z"
      ],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}